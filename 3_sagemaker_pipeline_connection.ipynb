{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bce1bd01",
   "metadata": {},
   "source": [
    "# MLOps 구축하기\n",
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c0da5a",
   "metadata": {},
   "source": [
    "## 1. SageMaker Studio 에서 Pipeline 실행\n",
    "\n",
    "사용 중인 region에서 SageMaker Studio를 실행합니다. Event Engine을 사용하시는 분들은 이미 SageMaker Studio가 생성되어 있기 때문에 AWS 콘솔에서 SageMaker에 있는 Studio를 클릭하여 **open Studio**를 클릭하여 환경으로 접속하게 됩니다.\n",
    "\n",
    "<p align=\"center\">\n",
    "<center><img src=\"./img/studio_notebook.png\" height=\"250\" width=\"850\" alt=\"\"><center>\n",
    "<br><br>\n",
    "</p>\n",
    "    \n",
    "SageMaker studio에 접속한 다음, SageMaker의 Pipeline을 생성하기 위해 Project를 생성합니다. **create project**를 선택한 다음 **MLOps template for model building and training**을 선택한 다음 **select project template**을 생성합니다.\n",
    "<p align=\"center\">\n",
    "<center><img src=\"./img/project_create.png\" height=\"250\" width=\"850\" alt=\"\"><center>\n",
    "<br><br>\n",
    "</p>   \n",
    "    \n",
    "원하시는 project 이름을 입력한 다음, **create project**를 클릭하여 project를 생성합니다.\n",
    "<p align=\"center\">\n",
    "<center><img src=\"./img/project_create_detail.png\" height=\"150\" width=\"550\" alt=\"\"><center>\n",
    "<br><br>\n",
    "</p>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c90a36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from sagemaker import get_execution_role\n",
    "from time import strftime\n",
    "import calendar\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6200f0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r\n",
    "print(f\"default_bucket : {default_bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c27130",
   "metadata": {},
   "source": [
    "### 1. 활용할 소스코드를 CodeCommit에 업로드 하기\n",
    "\n",
    "데이터 과학자가 개발한 소스코드를 S3 또는 CodeCommit, Github 등을 통해 플랫폼 엔지니어가 개발하는 플랫폼으로 전달할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7397a6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "iam_client = boto3.client('iam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba308a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "iam_client.attach_role_policy(\n",
    "    RoleName='AmazonSageMakerServiceCatalogProductsUseRole',\n",
    "    PolicyArn='arn:aws:iam::aws:policy/SecretsManagerReadWrite'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6066e934",
   "metadata": {},
   "outputs": [],
   "source": [
    "sts_client = boto3.client(\"sts\")\n",
    "account_id = sts_client.get_caller_identity()['Account']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7768029",
   "metadata": {},
   "outputs": [],
   "source": [
    "codecommit_client = boto3.client('codecommit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137e0453",
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_name = codecommit_client.list_repositories(\n",
    "    sortBy='lastModifiedDate',\n",
    "    order='descending' ## 'descending' ascending\n",
    ")\n",
    "\n",
    "for rep_item in rep_name['repositories']:\n",
    "    if 'informer-' in rep_item['repositoryName']:\n",
    "        repositoryName = rep_item['repositoryName']\n",
    "    elif 'informer2020' in rep_item['repositoryName']:\n",
    "        code_repositoryName = rep_item['repositoryName']\n",
    "\n",
    "        \n",
    "# repositoryName=rep_name['repositories'][0]['repositoryName']\n",
    "\n",
    "response = codecommit_client.get_repository(\n",
    "    repositoryName=repositoryName\n",
    ")\n",
    "\n",
    "code_response = codecommit_client.get_repository(\n",
    "    repositoryName=code_repositoryName\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd43e6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68194421",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_giturl = response['repositoryMetadata']['cloneUrlHttp']\n",
    "code_commit_repo = code_response['repositoryMetadata']['cloneUrlHttp']\n",
    "print(f\"pipeline_giturl : {pipeline_giturl}\\ncode_commit_repo : {code_commit_repo}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ba7368",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone $pipeline_giturl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befd187d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client = boto3.client(\"sagemaker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f7ec71",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_model_pkg = sm_client.list_model_package_groups(\n",
    "    SortBy='CreationTime',\n",
    "    SortOrder='Descending'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5344f7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package_group_name = res_model_pkg['ModelPackageGroupSummaryList'][0]['ModelPackageGroupName']\n",
    "model_package_group_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891f9abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./pipelines/informer/pipeline.py\n",
    "\"\"\"Example workflow pipeline script for abalone pipeline.\n",
    "\n",
    "                                               . -RegisterModel\n",
    "                                              .\n",
    "    Process-> Train -> Evaluate -> Condition .\n",
    "                                              .\n",
    "                                               . -(stop)\n",
    "\n",
    "Implements a get_pipeline(**kwargs) method.\n",
    "\"\"\"\n",
    "\n",
    "# sagemaker-experiments\n",
    "\n",
    "import os\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "import sagemaker.session\n",
    "\n",
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "\n",
    "\n",
    "import shutil\n",
    "\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "# from tqdm import tqdm\n",
    "from time import strftime\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "from sagemaker.model_metrics import (\n",
    "    MetricsSource,\n",
    "    ModelMetrics,\n",
    ")\n",
    "from sagemaker.processing import (\n",
    "    ProcessingInput,\n",
    "    ProcessingOutput,\n",
    "    ScriptProcessor,\n",
    ")\n",
    "from sagemaker.processing import FrameworkProcessor\n",
    "from sagemaker.workflow.conditions import ConditionLessThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import (\n",
    "    ConditionStep,\n",
    "    JsonGet,\n",
    ")\n",
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    "    ParameterFloat\n",
    ")\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.steps import (\n",
    "    ProcessingStep,\n",
    "    TrainingStep,\n",
    ")\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.workflow.steps import CacheConfig\n",
    "\n",
    "BASE_DIR = os.path.dirname(os.path.realpath(__file__))\n",
    "SOURCE_DIR = 'Informer2020'\n",
    "\n",
    "\n",
    "def get_sagemaker_client(region):\n",
    "    \"\"\"Gets the sagemaker client.\n",
    "\n",
    "        Args:\n",
    "            region: the aws region to start the session\n",
    "            default_bucket: the bucket to use for storing the artifacts\n",
    "\n",
    "        Returns:\n",
    "            `sagemaker.session.Session instance\n",
    "    \"\"\"\n",
    "    boto_session = boto3.Session(region_name=region)\n",
    "    sagemaker_client = boto_session.client(\"sagemaker\")\n",
    "    return sagemaker_client\n",
    "\n",
    "\n",
    "def get_session(region):\n",
    "    \"\"\"Gets the sagemaker session based on the region.\n",
    "\n",
    "    Args:\n",
    "        region: the aws region to start the session\n",
    "        default_bucket: the bucket to use for storing the artifacts\n",
    "\n",
    "    Returns:\n",
    "        `sagemaker.session.Session instance\n",
    "    \"\"\"\n",
    "\n",
    "    boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "    sagemaker_client = boto_session.client(\"sagemaker\")\n",
    "    runtime_client = boto_session.client(\"sagemaker-runtime\")\n",
    "    return sagemaker.session.Session(\n",
    "        boto_session=boto_session,\n",
    "        sagemaker_client=sagemaker_client,\n",
    "        sagemaker_runtime_client=runtime_client,\n",
    "#         default_bucket=default_bucket,\n",
    "    )\n",
    "\n",
    "def get_pipeline_custom_tags(new_tags, region, sagemaker_project_arn=None):\n",
    "    try:\n",
    "        sm_client = get_sagemaker_client(region)\n",
    "        response = sm_client.list_tags(\n",
    "            ResourceArn=sagemaker_project_arn)\n",
    "        project_tags = response[\"Tags\"]\n",
    "        for project_tag in project_tags:\n",
    "            new_tags.append(project_tag)\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting project tags: {e}\")\n",
    "    return new_tags\n",
    "\n",
    "###############################################################\n",
    "###############################################################\n",
    "### 2번 노트북 - '4. Experments 관리' 부터 복사해서 붙여넣기를 합니다. ###\n",
    "###############################################################\n",
    "###############################################################\n",
    "\n",
    "def get_pipeline(\n",
    "    region,\n",
    "    sagemaker_project_arn=None,\n",
    "    role=None,\n",
    "    default_bucket=None,\n",
    "    model_package_group_name=\"TimeSeries-Group\",\n",
    "    pipeline_name=\"InformerPipeline\",\n",
    "    base_job_prefix=\"informer\",\n",
    "):\n",
    "    \"\"\"Gets a SageMaker ML Pipeline instance working with on ts data.\n",
    "\n",
    "    Args:\n",
    "        region: AWS region to create and run the pipeline.\n",
    "        role: IAM role to create and run steps and pipeline.\n",
    "        default_bucket: the bucket to use for storing the artifacts\n",
    "\n",
    "    Returns:\n",
    "        an instance of a pipeline\n",
    "    \"\"\"\n",
    "    sagemaker_session = get_session(region)\n",
    "    default_bucket = sagemaker_session.default_bucket()\n",
    "    if role is None:\n",
    "        role = sagemaker.session.get_execution_role(sagemaker_session)\n",
    "\n",
    "    ### 4. Experiments 관리    \n",
    "    def create_experiment(experiment_name):\n",
    "        try:\n",
    "            sm_experiment = Experiment.load(experiment_name)\n",
    "        except:\n",
    "            sm_experiment = Experiment.create(experiment_name=experiment_name,\n",
    "                                              tags=[{'Key': 'modelname', 'Value': 'informer'}])\n",
    "\n",
    "\n",
    "    def create_trial(experiment_name, i_type, i_cnt, spot=False):\n",
    "        create_date = strftime(\"%m%d-%H%M%s\")\n",
    "        algo = 'informer'\n",
    "\n",
    "        spot = 's' if spot else 'd'\n",
    "        i_type = i_type[3:9].replace('.','-')\n",
    "\n",
    "        trial = \"-\".join([i_type,str(i_cnt),algo, spot])\n",
    "\n",
    "        sm_trial = Trial.create(trial_name=f'{experiment_name}-{trial}-{create_date}',\n",
    "                                experiment_name=experiment_name)\n",
    "\n",
    "        job_name = f'{sm_trial.trial_name}'\n",
    "        return job_name\n",
    "\n",
    "    \n",
    "    ### 5. 실험 설정\n",
    "    code_location = f's3://{default_bucket}/poc_informer/sm_codes'\n",
    "    output_path = f's3://{default_bucket}/poc_informer/output'         \n",
    "        \n",
    "        \n",
    "    metric_definitions = [\n",
    "        {'Name': 'Epoch', 'Regex': 'Epoch: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?),'},\n",
    "        {'Name': 'train_loss', 'Regex': 'Train Loss: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?),'},\n",
    "        {'Name': 'valid_loss', 'Regex': 'Valid Loss: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?),'},\n",
    "        {'Name': 'test_loss', 'Regex': 'Test Loss: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?),'},\n",
    "    ]        \n",
    "        \n",
    "    hyperparameters = {\n",
    "            'model' : 'informer', # model of experiment, options: [informer, informerstack, informerlight(TBD)]\n",
    "            'data' : 'ETTh1', # data\n",
    "            'root_path' : 'ETT-small/', # root path of data file\n",
    "            'data_path' : 'ETTh1.csv', # data file\n",
    "            'features' : 'M', # forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate\n",
    "            'target' : 'OT', # target feature in S or MS task\n",
    "            'freq' : 'h', # freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h\n",
    "            'checkpoints' : 'informer_checkpoints', # location of model checkpoints\n",
    "\n",
    "            'seq_len' : 96, # input sequence length of Informer encoder\n",
    "            'label_len' : 48, # start token length of Informer decoder\n",
    "            'pred_len' : 24, # prediction sequence length\n",
    "            # Informer decoder input: concat[start token series(label_len), zero padding series(pred_len)]\n",
    "\n",
    "            'enc_in' : 7, # encoder input size\n",
    "            'dec_in' : 7, # decoder input size\n",
    "            'c_out' : 7, # output size\n",
    "            'factor' : 5, # probsparse attn factor\n",
    "            'd_model' : 512, # dimension of model\n",
    "            'n_heads' : 8, # num of heads\n",
    "            'e_layers' : 2, # num of encoder layers\n",
    "            'd_layers' : 1, # num of decoder layers\n",
    "            'd_ff' : 2048, # dimension of fcn in model\n",
    "            'dropout' : 0.05, # dropout\n",
    "            'attn' : 'prob', # attention used in encoder, options:[prob, full]\n",
    "            'embed' : 'timeF', # time features encoding, options:[timeF, fixed, learned]\n",
    "            'activation' : 'gelu', # activation\n",
    "            'distil' : True, # whether to use distilling in encoder\n",
    "            'output_attention' : False, # whether to output attention in ecoder\n",
    "            'mix' : True,\n",
    "            'padding' : 0,\n",
    "            'freq' : 'h',\n",
    "            'do_predict' : True,\n",
    "            'batch_size' : 32,\n",
    "            'learning_rate' : 0.0001,\n",
    "            'loss' : 'mse',\n",
    "            'lradj' : 'type1',\n",
    "            'use_amp' : False, # whether to use automatic mixed precision training\n",
    "\n",
    "            'num_workers' : 0,\n",
    "            'itr' : 1,\n",
    "            'train_epochs' : 1,  ## Training epochs\n",
    "            'patience' : 3,\n",
    "            'des' : 'exp',\n",
    "            'use_multi_gpu' : True\n",
    "        }        \n",
    "     \n",
    "    experiment_name = 'informer-poc-exp1'                                                          ### <== 1. Experiments 이름 수정\n",
    "    distribution = None\n",
    "    do_spot_training = True\n",
    "    max_wait = None\n",
    "    max_run = 1*30*60\n",
    "        \n",
    "    instance_type=\"ml.m5.xlarge\"                                                                   \n",
    "    instance_count=1        \n",
    "    \n",
    "    \n",
    "    ### 6. Pipeline parameters, checkpoints와 데이터 위치 설정\n",
    "    #### 6-1. Pipeline parameters\n",
    "    train_instance_param = ParameterString(\n",
    "        name=\"TrainingInstance\",\n",
    "        default_value=\"ml.c5.4xlarge\",                                                             ### <== 2. Instance 타입, 개수 수정\n",
    "    )\n",
    "\n",
    "    train_count_param = ParameterInteger(\n",
    "        name=\"TrainingInstanceCount\",\n",
    "        default_value=1\n",
    "    )\n",
    "\n",
    "    model_approval_status = ParameterString(\n",
    "        name=\"ModelApprovalStatus\", default_value=\"PendingManualApproval\"\n",
    "    )\n",
    "    \n",
    "    #### 6-2. checkpoints와 데이터 위치 설정\n",
    "    image_uri = None\n",
    "    train_job_name = 'informer'\n",
    "\n",
    "    if do_spot_training:\n",
    "        max_wait = max_run\n",
    "\n",
    "    print(\"train_job_name : {} \\ntrain_instance_type : {} \\ntrain_instance_count : {} \\nimage_uri : {} \\ndistribution : {}\".format(train_job_name, train_instance_param.default_value, train_count_param.default_value, image_uri, distribution))    \n",
    "\n",
    "    \n",
    "    prefix = 'ETDataset'\n",
    "    inputs = f's3://{default_bucket}/dataset/{prefix}'\n",
    "\n",
    "    source_dir = 'Informer2020'                                                                    ### <== 3. git repository내의 소스 코드 위치 \n",
    "    checkpoint_s3_uri = f's3://{default_bucket}/poc_informer/checkpoints'      \n",
    "    \n",
    "    \n",
    "    #### 6-3. Git 설정 (Secret Manager 활용)\n",
    "    def get_secret(secret_name):\n",
    "        secret = {}\n",
    "        # Create a Secrets Manager client\n",
    "        session = boto3.session.Session()\n",
    "        client = session.client(\n",
    "            service_name='secretsmanager'\n",
    "        )\n",
    "\n",
    "        # In this sample we only handle the specific exceptions for the 'GetSecretValue' API.\n",
    "        # See https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_GetSecretValue.html\n",
    "        # We rethrow the exception by default.\n",
    "\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId=secret_name\n",
    "        )\n",
    "\n",
    "        if 'SecretString' in get_secret_value_response:\n",
    "            secret = get_secret_value_response['SecretString']\n",
    "            secret = json.loads(secret)\n",
    "        else:\n",
    "            print(\"secret is not defined. Checking the Secrets Manager\")\n",
    "\n",
    "        return secret        \n",
    "        \n",
    "    ### CodeCommit의 Credentials이 저장된 secret_name 사용이 필요합니다.\n",
    "    sec_client = boto3.client('secretsmanager')\n",
    "    secret_name = sec_client.list_secrets(SortOrder='desc')['SecretList'][0]['ARN']               ### <== 4. git credentials이 있는 secret manage 이름 수정\n",
    "     \n",
    "    secret=get_secret(secret_name)\n",
    "\n",
    "    git_config = {'repo': 'https://git-codecommit.${region}.amazonaws.com/v1/repos/informer2020', ### <== 5. git repository 위치 수정\n",
    "                  'branch': 'main',\n",
    "                  'username': secret['username'],\n",
    "                  'password': secret['password']}\n",
    "        \n",
    "        \n",
    "    ### 7. 학습을 위한 Estimator 선언\n",
    "    create_experiment(experiment_name)\n",
    "    job_name = create_trial(experiment_name, instance_type, instance_count, spot=do_spot_training)\n",
    "\n",
    "\n",
    "    estimator = PyTorch(\n",
    "        entry_point='main_informer.py',\n",
    "        source_dir=source_dir,\n",
    "        git_config=git_config,\n",
    "        role=role,\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        framework_version='1.10',\n",
    "        py_version='py38',\n",
    "        instance_count=train_count_param,    ## Parameter 값으로 변경\n",
    "        instance_type=train_instance_param,  ## Parameter 값으로 변경\n",
    "        volume_size=256,\n",
    "        code_location = code_location,\n",
    "        output_path=output_path,\n",
    "        hyperparameters=hyperparameters,\n",
    "        distribution=distribution,\n",
    "        metric_definitions=metric_definitions,\n",
    "        max_run=max_run,\n",
    "        checkpoint_s3_uri=checkpoint_s3_uri,\n",
    "        use_spot_instances=do_spot_training,  # spot instance 활용\n",
    "        max_wait=max_wait,\n",
    "        base_job_name=f\"training-{job_name}\",\n",
    "        disable_profiler=True,\n",
    "        debugger_hook_config=False,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    ### 8. Training 단계 선언    \n",
    "    from sagemaker.workflow.steps import CacheConfig\n",
    "\n",
    "    cache_config = CacheConfig(enable_caching=True, \n",
    "                               expire_after=\"7d\")        \n",
    "        \n",
    "        \n",
    "    training_step = TrainingStep(\n",
    "        name=\"InformerTrain\",\n",
    "        estimator=estimator,\n",
    "        inputs={\n",
    "            \"training\": sagemaker.inputs.TrainingInput(\n",
    "                s3_data=inputs\n",
    "            )\n",
    "        },\n",
    "        cache_config=cache_config\n",
    "    )        \n",
    "        \n",
    "    \n",
    "    ### 9. Evaluation 단계 - output에서 압축풀어 test_report.json 가져오기\n",
    "    framework_processor = FrameworkProcessor(\n",
    "        PyTorch,\n",
    "        framework_version=\"1.10\",\n",
    "        py_version='py38',\n",
    "        role=role,\n",
    "        instance_count=1,\n",
    "        instance_type=\"ml.c4.xlarge\",\n",
    "        code_location=code_location,\n",
    "        base_job_name=f\"generatingreport-{job_name}\",  # choose any name\n",
    "    )     \n",
    "        \n",
    "    model_input = ProcessingInput(\n",
    "        source=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "        destination=\"/opt/ml/processing/model\",\n",
    "    )      \n",
    "        \n",
    "    test_report = PropertyFile(\n",
    "        name=\"TestReport\",\n",
    "        output_name=\"result\",\n",
    "        path=\"test_report.json\",\n",
    "    )\n",
    "        \n",
    "    run_args = framework_processor.get_run_args(\n",
    "        code=\"postprocess.py\",\n",
    "        source_dir=\"Informer2020\",\n",
    "        git_config=git_config,\n",
    "        inputs=[model_input],\n",
    "        outputs=[\n",
    "            ProcessingOutput(output_name=\"result\", source=\"/opt/ml/processing/result\")\n",
    "        ],\n",
    "        job_name=f\"process-step-{job_name}\"\n",
    "    )        \n",
    "        \n",
    "\n",
    "    postprocessing_step = ProcessingStep(\n",
    "        name=\"PostProcessingforInformer\",  # choose any name\n",
    "        processor=framework_processor,\n",
    "        inputs=run_args.inputs,\n",
    "        outputs=run_args.outputs,\n",
    "        code=run_args.code,\n",
    "        property_files=[test_report],\n",
    "        cache_config=cache_config\n",
    "    )\n",
    "        \n",
    "\n",
    "    ### 10. Model 등록 단계\n",
    "    model_package_group_name = 'mlops-test-informer-p-XXXXXX'                                           ### <== 6. model package group 이름 수정\n",
    "    \n",
    "    model_metrics = ModelMetrics(\n",
    "        model_statistics=MetricsSource(\n",
    "            s3_uri=\"{}/test_report.json\".format(\n",
    "                postprocessing_step.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"],\n",
    "            ),\n",
    "            content_type=\"application/json\",\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    register_step = RegisterModel(\n",
    "        name=\"InformerRegisterModel\",\n",
    "        estimator=estimator,\n",
    "        model_data=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "        content_types=[\"text/csv\"],\n",
    "        response_types=[\"text/csv\"],\n",
    "        inference_instances=[\"ml.m5.xlarge\"],\n",
    "        transform_instances=[\"ml.m5.xlarge\"],\n",
    "        model_package_group_name=model_package_group_name,\n",
    "        approval_status=model_approval_status,\n",
    "        model_metrics=model_metrics,\n",
    "    )    \n",
    "\n",
    "    \n",
    "    ### 11. Condition 단계\n",
    "    cond_lte = ConditionLessThanOrEqualTo(  # You can change the condition here\n",
    "        left=JsonGet(\n",
    "            step=postprocessing_step,\n",
    "            property_file=test_report,\n",
    "            json_path=\"regression_metrics.mse.value\",  # This should follow the structure of your report_dict defined in the postprocess.py file.\n",
    "        ),\n",
    "        right=1.0,  # You can change the threshold here\n",
    "    )\n",
    "    cond_step = ConditionStep(\n",
    "        name=\"TestMSECond\",\n",
    "        conditions=[cond_lte],\n",
    "        if_steps=[register_step],\n",
    "        else_steps=[],\n",
    "    )\n",
    "    \n",
    "    ### 12. Pipeline 수행\n",
    "    pipeline = Pipeline(\n",
    "        name=\"ts-prediction-informer-pipeline\",\n",
    "        parameters=[train_instance_param, train_count_param, model_approval_status],\n",
    "        steps=[\n",
    "            training_step,\n",
    "            postprocessing_step,\n",
    "            cond_step\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7745215",
   "metadata": {},
   "source": [
    "작성한 pipeline.py와 함께 **pipelines** 폴더와 **codebuild-buildspec.yml** 파일을 SageMaker Project로 생성한 CodeCommit의 repository 안의 값과 대체합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab041f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "repositoryName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c5e82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r ./pipelines ./$repositoryName/\n",
    "!cp -r ./codebuild-buildspec.yml ./$repositoryName/codebuild-buildspec.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4004f3",
   "metadata": {},
   "source": [
    "\n",
    "이후 SageMaker Studio에서 모든 소스코드를 commit한 다음 push를 하면 이후 자동으로 SageMaker Pipeline은 실행이 됩니다. 실행되는 모습은 SageMaker Pipeline에서 확인하거나 Codepipeline에서 확인하실 수 있습니다.\n",
    "<p align=\"center\">\n",
    "<center><img src=\"./img/pipeline_codecommit.png\" height=\"250\" width=\"850\" alt=\"\"><center>\n",
    "<br><br>\n",
    "</p>\n",
    "<p align=\"center\">\n",
    "<center><img src=\"./img/code_commit.png\" height=\"250\" width=\"750\" alt=\"\"><center>\n",
    "<br><br>\n",
    "</p>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e9d81f",
   "metadata": {},
   "source": [
    "### 2. MLOps에서 활용할 Policy 설정하기\n",
    "\n",
    "해당 HOL에서 구현할 아키텍처에 필요한 managed policy를 아래와 같이 정의합니다. Role을 별도 생성하셔도 되지만 HOL의 편의성을 위해 SageMaker Notebook/Studio와 동일한 Role에 policy를 추가하여 계속 활용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ddcdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "role=get_execution_role()\n",
    "base_role_name=role.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4955420a",
   "metadata": {},
   "outputs": [],
   "source": [
    "iam_client.attach_role_policy(\n",
    "    RoleName=base_role_name,\n",
    "    PolicyArn='arn:aws:iam::aws:policy/AmazonEventBridgeFullAccess'\n",
    ")\n",
    "iam_client.attach_role_policy(\n",
    "    RoleName=base_role_name,\n",
    "    PolicyArn='arn:aws:iam::aws:policy/AWSLambda_FullAccess'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21cb108",
   "metadata": {},
   "source": [
    "### 3. SageMaker Studio의 설정값에서 model_package_group_name 가져오기\n",
    "이 작업을 진행하기 이전에 SageMaker Studio에서 Project 생성이 필요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ae158e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "project_name=sm_client.list_projects(\n",
    "    SortBy='CreationTime',\n",
    "    SortOrder='Descending')['ProjectSummaryList'][0]['ProjectName']\n",
    "project_name\n",
    "\n",
    "project_response=sm_client.describe_project(ProjectName=project_name)\n",
    "model_package_group_name = project_response['ProjectName']+\"-\"+project_response['ProjectId']\n",
    "model_package_group_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa907c6",
   "metadata": {},
   "source": [
    "### 4. Create Amazon EventBridge Rule\n",
    "\n",
    "model registry에서 모델이 **Approved**되었을 때 이벤트 트리거를 만들기 위한 설정을 Amazon EventBridge Rule을 이용하여 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583d87d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_client = boto3.client('events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92c1fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "eventpattern = json.dumps(\n",
    "    {\n",
    "      \"source\": [\"aws.sagemaker\"],\n",
    "      \"detail-type\": [\"SageMaker Model Package State Change\"],\n",
    "      \"detail\": {\n",
    "        \"ModelPackageGroupName\": [f\"{model_package_group_name}\"],\n",
    "        \"ModelApprovalStatus\": [\"Approved\"]\n",
    "      }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3c6652",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(7)\n",
    "rule_name = 'informer_model_package_state'\n",
    "event_rule = event_client.put_rule(\n",
    "    Name=rule_name,\n",
    "    EventPattern=eventpattern,\n",
    "    State='ENABLED',\n",
    "    Description='This is after the approval update for the Informer model',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50f9c7a",
   "metadata": {},
   "source": [
    "### 5. Lambda function 생성\n",
    "\n",
    "EventBridge 에서 Rule 만족하는 이벤트가 발생했을 때 실행되는 Lambda Function을 정의합니다. Lambda Function 은 테스트 데이터를 예측하는 Batch transform job을 수행하게 됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858798f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir batch_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582f1b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile batch_transform/sm_batch_transform.py\n",
    "\n",
    "import json\n",
    "import boto3\n",
    "import os\n",
    "from time import strftime\n",
    "\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \"\"\"\n",
    "    모델 레지스트리에서 최신 버전의 모델 승인 상태를 변경하는 람다 함수.\n",
    "    \"\"\"\n",
    "    \n",
    "#     try:\n",
    "    ##############################################\n",
    "    # 람다 함수는 Event Bridge의 패턴 정보를 event 개체를 통해서 받습니다.\n",
    "    ##############################################   \n",
    "    print(f\"event : {event}\")\n",
    "    model_package_arn = event['detail'][\"ModelPackageArn\"]\n",
    "    model_package_group_name = event['detail'][\"ModelPackageGroupName\"]\n",
    "    print(\"model_package_arn: \", model_package_arn)      \n",
    "    print(\"model_package_group_name: \", model_package_group_name)\n",
    "\n",
    "    ### 환경 변수\n",
    "    secret_name = os.environ[\"SECRET_NAME\"]\n",
    "    code_commit_repo = os.environ[\"CODE_COMMIT_REPO\"]\n",
    "    default_bucket = os.environ[\"DEFAULT_BUCKET\"]\n",
    "    role = os.environ[\"ROLE\"] \n",
    "\n",
    "    secret=get_secret(secret_name)\n",
    "\n",
    "    git_config = {'repo': code_commit_repo,\n",
    "                  'branch': 'main',\n",
    "                  'username': secret['username'],\n",
    "                  'password': secret['password']}\n",
    "\n",
    "    code_location = f's3://{default_bucket}/poc_informer/sm_codes'\n",
    "\n",
    "    sess = boto3.Session()\n",
    "    sm = sess.client('sagemaker')\n",
    "    model_pkg = sm.describe_model_package(ModelPackageName=model_package_arn)\n",
    "\n",
    "    model = PyTorchModel(\n",
    "        entry_point='predictor.py',\n",
    "        source_dir='Informer2020',\n",
    "        git_config=git_config,\n",
    "        code_location=code_location,\n",
    "        model_data=model_pkg['InferenceSpecification']['Containers'][0]['ModelDataUrl'],\n",
    "        role=role,\n",
    "        framework_version=\"1.10\",\n",
    "        py_version=\"py38\"\n",
    "    )\n",
    "\n",
    "    transformer= model.transformer(\n",
    "        instance_count=1,\n",
    "        instance_type='ml.m5.xlarge',\n",
    "        assemble_with=\"Line\",\n",
    "        output_path=f\"s3://{default_bucket}/poc_informer/batch_result\",\n",
    "        env={'default_bucket': default_bucket}\n",
    "    )\n",
    "\n",
    "    from time import strftime\n",
    "\n",
    "    job_name=model_package_group_name+\"-\"+strftime(\"%m%d-%H%M%s\")\n",
    "\n",
    "    batch_transform = transformer.transform(\n",
    "        data=f's3://{default_bucket}/dataset/ETDataset/ETT-small/ETTh1.csv',\n",
    "        data_type='S3Prefix',\n",
    "        content_type='text/csv',\n",
    "        split_type='Line',\n",
    "        job_name=f\"tranform-{job_name}\",\n",
    "        wait=False\n",
    "    )\n",
    "\n",
    "    return_msg = f\"Starting Batch Transform\"\n",
    "\n",
    "    ##############################################        \n",
    "    # 람다 함수의 리턴 정보를 구성하고 리턴 합니다.\n",
    "    ##############################################        \n",
    "\n",
    "    return {\n",
    "        \"statusCode\": 200,\n",
    "        \"body\": json.dumps(return_msg),\n",
    "        \"other_key\": \"example_value\",\n",
    "    }\n",
    "\n",
    "#     except BaseException as error:\n",
    "#         return_msg = f\"There is no model_package_group_name {model_package_group_name}\"                \n",
    "#         error_msg = f\"An exception occurred: {error}\"\n",
    "#         print(error_msg)    \n",
    "#         return {\n",
    "#             \"statusCode\": 500,\n",
    "#             \"body\": json.dumps(return_msg),\n",
    "#             \"other_key\": \"example_value\",\n",
    "#         }        \n",
    "        \n",
    "def get_secret(secret_name):\n",
    "    secret = {}\n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager'\n",
    "    )\n",
    "\n",
    "    # In this sample we only handle the specific exceptions for the 'GetSecretValue' API.\n",
    "    # See https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_GetSecretValue.html\n",
    "    # We rethrow the exception by default.\n",
    "\n",
    "    get_secret_value_response = client.get_secret_value(\n",
    "        SecretId=secret_name\n",
    "    )\n",
    "        \n",
    "    if 'SecretString' in get_secret_value_response:\n",
    "        secret = get_secret_value_response['SecretString']\n",
    "        secret = json.loads(secret)\n",
    "    else:\n",
    "        print(\"secret is not defined. Checking the Secrets Manager\")\n",
    "\n",
    "    return secret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d6f831",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile batch_transform/Dockerfile\n",
    "\n",
    "# Define function directory\n",
    "ARG FUNCTION_DIR=\"/function\"\n",
    "\n",
    "FROM python:buster as build-image\n",
    "\n",
    "# Install aws-lambda-cpp build dependencies\n",
    "RUN apt-get update && \\\n",
    "  apt-get install -y \\\n",
    "  g++ \\\n",
    "  make \\\n",
    "  cmake \\\n",
    "  unzip \\\n",
    "  git \\\n",
    "  libcurl4-openssl-dev\n",
    "\n",
    "# Include global arg in this stage of the build\n",
    "ARG FUNCTION_DIR\n",
    "# Create function directory\n",
    "RUN mkdir -p ${FUNCTION_DIR}\n",
    "\n",
    "# Copy function code\n",
    "COPY sm_batch_transform.py ${FUNCTION_DIR}\n",
    "# COPY git_lambda ${FUNCTION_DIR}/git_lambda\n",
    "# COPY yolov5 ${FUNCTION_DIR}/yolov5\n",
    "\n",
    "# Install the runtime interface client\n",
    "RUN pip install \\\n",
    "        --target ${FUNCTION_DIR} \\\n",
    "        awslambdaric sagemaker smdebug sagemaker-experiments\n",
    "\n",
    "# Multi-stage build: grab a fresh copy of the base image\n",
    "FROM python:buster\n",
    "\n",
    "# Include global arg in this stage of the build\n",
    "ARG FUNCTION_DIR\n",
    "# Set working directory to function root directory\n",
    "WORKDIR ${FUNCTION_DIR}\n",
    "\n",
    "# Copy in the build image dependencies\n",
    "COPY --from=build-image ${FUNCTION_DIR} ${FUNCTION_DIR}\n",
    "\n",
    "ENTRYPOINT [ \"/usr/local/bin/python\", \"-m\", \"awslambdaric\" ]\n",
    "CMD [ \"sm_batch_transform.lambda_handler\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c6c884",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ./batch_transform\n",
    "echo $(pwd)\n",
    "container_name=lambda-informer-batchtransform\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-2}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${container_name}:1.0\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${container_name}\" > /dev/null 2>&1\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${container_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# # Get the login command from ECR and execute it directly\n",
    "# $(aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin \"763104351884.dkr.ecr.us-west-2.amazonaws.com\")\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "docker build -f Dockerfile -t ${fullname} .\n",
    "# docker tag ${container_name} ${fullname}\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08f79ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_session = boto3.session.Session()\n",
    "region = my_session.region_name\n",
    "\n",
    "repo_name=f\"{account_id}.dkr.ecr.{region}.amazonaws.com/lambda-informer-batchtransform:1.0\"\n",
    "repo_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b8af19",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_client = boto3.client('lambda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356e1fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_trust_policy=json.dumps({\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Principal\": {\n",
    "        \"Service\": \"lambda.amazonaws.com\"\n",
    "      },\n",
    "      \"Action\": \"sts:AssumeRole\"\n",
    "    }\n",
    "  ]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1e601f",
   "metadata": {},
   "outputs": [],
   "source": [
    "role_name='lambda-assume-role_'+ strftime(\"%m%d-%H%M%s\")\n",
    "try:\n",
    "    for role_list in iam_client.list_roles()['Roles']:\n",
    "        pre_role_name = role_list['RoleName']\n",
    "        if pre_role_name.split(\"_\")[0] in ['lambda-assume-role']:\n",
    "            iam_client.detach_role_policy(\n",
    "                RoleName=pre_role_name,\n",
    "                PolicyArn='arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole'\n",
    "            )\n",
    "            iam_client.detach_role_policy(\n",
    "                RoleName=pre_role_name,\n",
    "                PolicyArn='arn:aws:iam::aws:policy/AmazonSageMakerFullAccess'\n",
    "            )\n",
    "            iam_client.delete_role(RoleName=pre_role_name)\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    lambda_role = iam_client.create_role(\n",
    "        RoleName=role_name,\n",
    "        AssumeRolePolicyDocument=lambda_trust_policy\n",
    "    )\n",
    "    iam_client.attach_role_policy(\n",
    "        RoleName=role_name,\n",
    "        PolicyArn='arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole'\n",
    "    )\n",
    "    iam_client.attach_role_policy(\n",
    "        RoleName=role_name,\n",
    "        PolicyArn='arn:aws:iam::aws:policy/AmazonSageMakerFullAccess'\n",
    "    )\n",
    "    iam_client.attach_role_policy(\n",
    "        RoleName=role_name,\n",
    "        PolicyArn='arn:aws:iam::aws:policy/SecretsManagerReadWrite'\n",
    "    )\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70220836",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CodeCommit의 Credentials이 저장된 secret_name 사용이 필요합니다.\n",
    "sec_client = boto3.client('secretsmanager')\n",
    "secret_name = sec_client.list_secrets(SortOrder='desc')['SecretList'][0]['ARN']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec69e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_name='informer-batch-transform'\n",
    "try:\n",
    "    lambda_client.delete_function(FunctionName=lambda_name)\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    lambda_response = lambda_client.create_function(\n",
    "        FunctionName=lambda_name,\n",
    "        Role=lambda_role['Role']['Arn'],\n",
    "        Code={\n",
    "            'ImageUri': repo_name\n",
    "        },\n",
    "        PackageType='Image',\n",
    "        Description='batch transform of the latest version-based infomer model',\n",
    "        Timeout=600,\n",
    "        MemorySize=512,\n",
    "        Environment={\n",
    "          'Variables': {\n",
    "              \"SECRET_NAME\" : secret_name,\n",
    "              \"CODE_COMMIT_REPO\" : code_commit_repo,\n",
    "              \"DEFAULT_BUCKET\" : default_bucket,\n",
    "              \"ROLE\" : role\n",
    "          }\n",
    "      }\n",
    "    )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8c306d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_permission_response = lambda_client.add_permission(\n",
    "    FunctionName=lambda_name,\n",
    "    StatementId='InvokeLambdaFunction',\n",
    "    Action='lambda:InvokeFunction',\n",
    "    Principal=\"events.amazonaws.com\",\n",
    "    SourceArn=event_rule['RuleArn'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535fc09d",
   "metadata": {},
   "source": [
    "Amazon EventBridge에 위에서 생성한 Lambda function을 타켓으로 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fbc994",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_client.put_targets(\n",
    "    Rule=rule_name,\n",
    "    Targets=[\n",
    "        {\n",
    "            'Id': 'Target0',\n",
    "            'Arn': lambda_response['FunctionArn']\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446f189f",
   "metadata": {},
   "source": [
    "### 6. Studio에서 실행하기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24323e36",
   "metadata": {},
   "source": [
    "SageMaker Pipelines의 실행 과정은 SageMaker Studio를 보면 아래와 같이 UI로 확인이 가능합니다.\n",
    "<p align=\"center\">\n",
    "<center><img src=\"./img/mlops_exe.png\" height=\"350\" width=\"750\" alt=\"\"><center>\n",
    "<br><br>\n",
    "</p>  \n",
    "    \n",
    "SageMaker Pipeline이 수행된 다음에는 학습된 model은 model registry에 버전별로 등록이 됩니다. 등록된 model을 **Approval**가 하게되면 앞서 설정한 EventBridge가 해당 이벤트를 rule에서 판단하게 되고 이후 테스트 데이터를 예측하는 Batch transform job을 수행하는 Lambda가 시작됩니다. \n",
    "<p align=\"center\">\n",
    "<center><img src=\"./img/model_registry.png\" height=\"350\" width=\"750\" alt=\"\"><center>\n",
    "<br><br>\n",
    "</p>      \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456a2d5d",
   "metadata": {},
   "source": [
    "### 7. QuickSight 생성하기\n",
    "\n",
    "AWS 콘솔에서 QuickSight 서비스를 생성합니다. 기본 설정에서 하단의 continue 버튼을 클릭합니다.\n",
    "<p align=\"center\">\n",
    "<center><img src=\"./img/quicksight_start.png\" height=\"250\" width=\"550\" alt=\"\"><center>\n",
    "<br><br>\n",
    "</p>  \n",
    " \n",
    "- Region은 편의성을 위해 현재 사용 중인 리전을 선택합니다. \n",
    "- 다음 Account Name은 unique한 이름으로 설정합니다. \n",
    "- 이메일 주소를 넣습니다.\n",
    "- Amazon S3를 선택한 다음 default_bucket 버킷명을 찾아서 선택하고 Finish 버튼을 클릭합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bd7f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(default_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84572e1",
   "metadata": {},
   "source": [
    "    \n",
    "<p align=\"center\">\n",
    "<center><img src=\"./img/quicksight_setting.png\" height=\"750\" width=\"1050\" alt=\"\"><center>\n",
    "<br><br>\n",
    "</p>  \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1d7263",
   "metadata": {},
   "source": [
    "### 8. QuickSight에서 사용할 manifest_file 생성하기\n",
    "\n",
    "quicksight에서 사용할 manifest_file 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb672d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest_file = {\n",
    "                \"fileLocations\": [\n",
    "                    {\n",
    "                        \"URIPrefixes\": [\n",
    "                            f\"s3://{default_bucket}/poc_informer/batch_result/\"\n",
    "                        ]\n",
    "                    }\n",
    "                ],\n",
    "                \"globalUploadSettings\": {\n",
    "                    \"format\": \"CSV\",\n",
    "                    \"delimiter\": \",\",\n",
    "                    \"textqualifier\": \"\\\"\",\n",
    "                    \"containsHeader\": \"true\"\n",
    "                }\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dd9e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./quicksight/manifest_file.json\", 'w', encoding=\"utf-8\") as f:\n",
    "    json.dump(manifest_file, f, indent=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015039c7",
   "metadata": {},
   "source": [
    "   \n",
    "왼쪽 메뉴에서 **Datasets**를 선택한 다음 오른쪽 상단의 **New dataset**을 선택합니다. 이후 **S3**를 선택하면 아래와 같이 팝업 창이 뜨고, 원하는 이름으로 **data source name**을 설정한 다음, **upload a manifest file**에는 위에 copy한 S3 주소를 입력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0952453",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync ./quicksight/prediction_result s3://$default_bucket/poc_informer/batch_result/ --quiet\n",
    "!aws s3 cp ./quicksight/manifest_file.json s3://$default_bucket/poc_informer/quick_sight/ --quiet\n",
    "print(f\"s3://{default_bucket}/poc_informer/quick_sight/manifest_file.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224cacc8",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<center><img src=\"./img/quicksight_dataset.png\" height=\"750\" width=\"1050\" alt=\"\"><center>\n",
    "<br><br>\n",
    "</p>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd277729",
   "metadata": {},
   "source": [
    "앞에서 생성한 dataset에서 **visualize**를 선택하면 아래와 같이 그래프 생성이 가능합니다. **date**는 X axis로, **OT (Ground Truth)**, **Prediction**는 Value로 선택한 다음, date의 간격을 **Hour**로 변경해 줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960e4b22",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<center><img src=\"./img/quicksight_vis.png\" height=\"750\" width=\"1050\" alt=\"\"><center>\n",
    "<br><br>\n",
    "</p>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f49b983",
   "metadata": {},
   "source": [
    "QuickSight에 관련된 자세한 실습은 [QuickSight Workshop](https://learnquicksight.workshop.aws/en/author-workshop/0.prerequisites.html) 에서 수행해 보시기 바랍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4df440f",
   "metadata": {},
   "source": [
    "### 9. quicksight spice dataset을 refresh하기 (Optional)\n",
    "\n",
    "모델이 새롭게 업데이트된 다음 다시 예측을 수행하게 되면 새롭게 생성된 결과 CSV 파일을 다시 업데이트하여 QuickSight Figure을 업데이트해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e717cc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_client = boto3.client(\"quicksight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067ad5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = qs_client.list_data_sets(AwsAccountId=account_id)\n",
    "\n",
    "# filter out your datasets using a prefix. All my datasets have chicago_crimes as their prefix\n",
    "datasets_ids = [summary[\"DataSetId\"] for summary in res[\"DataSetSummaries\"]]\n",
    "ingestion_ids = []\n",
    "\n",
    "for dataset_id in datasets_ids:\n",
    "    try:\n",
    "        ingestion_id = str(calendar.timegm(time.gmtime()))\n",
    "#         ingestion_id = str(uuid.uuid4())\n",
    "        qs_client.create_ingestion(DataSetId=dataset_id, IngestionId=ingestion_id,\n",
    "                                             AwsAccountId=account_id)\n",
    "        ingestion_ids.append(ingestion_id)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "\n",
    "for ingestion_id, dataset_id in zip(ingestion_ids, datasets_ids):\n",
    "    while True:\n",
    "        response = qs_client.describe_ingestion(DataSetId=dataset_id,\n",
    "                                             IngestionId=ingestion_id,\n",
    "                                             AwsAccountId=account_id)\n",
    "        if response['Ingestion']['IngestionStatus'] in ('INITIALIZED', 'QUEUED', 'RUNNING'):\n",
    "            time.sleep(5)     #change sleep time according to your dataset size\n",
    "        elif response['Ingestion']['IngestionStatus'] == 'COMPLETED':\n",
    "            print(\"refresh completed. RowsIngested {0}, RowsDropped {1}, IngestionTimeInSeconds {2}, IngestionSizeInBytes {3}\".format(\n",
    "                response['Ingestion']['RowInfo']['RowsIngested'],\n",
    "                response['Ingestion']['RowInfo']['RowsDropped'],\n",
    "                response['Ingestion']['IngestionTimeInSeconds'],\n",
    "                response['Ingestion']['IngestionSizeInBytes']))\n",
    "            break\n",
    "        else:\n",
    "            print(\"refresh failed for {0}! - status {1}\".format(dataset_id, response['Ingestion']['IngestionStatus']))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be42803b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f180a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
