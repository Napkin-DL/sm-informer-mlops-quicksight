{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Pipeline\n",
    "--------------\n",
    "\n",
    "ML workflow의 각 단계를 수동으로 수행했고, 이제는 모델 학습과 모델 registry에 등록하는 과정에 대한 파이프라인을 만듭니다. [Amazon SageMaker 모델 구축 파이프라인](https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/pipelines.html)\n",
    "\n",
    "<p align=\"center\">\n",
    "<center><img src=\"./img/mdp_how_it_works.png\" height=\"250\" width=\"850\" alt=\"\"><center>\n",
    "<br><br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. import 패키지 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import sagemaker\n",
    "# import splitfolders\n",
    "\n",
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "\n",
    "# import wget\n",
    "# import tarfile\n",
    "import shutil\n",
    "\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "# from tqdm import tqdm\n",
    "from time import strftime\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Experiments 관리\n",
    "\n",
    "Amazon SageMaker에는 실험을 관리할 수 있는 [SageMaker Experiments](https://aws.amazon.com/ko/blogs/aws/amazon-sagemaker-experiments-organize-track-and-compare-your-machine-learning-trainings/) 서비스가 있습니다. 반복적인 실험에 대해 로깅을 남기기 위한 실험 이름 (create_experiment)과 trial (create_trial) 이름을 설정하는 함수입니다. <br> 이러한 메타 정보를 이용하여 향후 ML의 실험 관리가 용이해 질 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_experiment(experiment_name):\n",
    "    try:\n",
    "        sm_experiment = Experiment.load(experiment_name)\n",
    "    except:\n",
    "        sm_experiment = Experiment.create(experiment_name=experiment_name,\n",
    "                                          tags=[\n",
    "                                              {\n",
    "                                                  'Key': 'modelname',\n",
    "                                                  'Value': 'informer'\n",
    "                                              },\n",
    "                                          ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trial(experiment_name, set_param, i_type, i_cnt, spot):\n",
    "    create_date = strftime(\"%m%d-%H%M%s\")\n",
    "    \n",
    "    algo = 'dp'\n",
    "    \n",
    "    spot = 's' if spot else 'd'\n",
    "    i_tag = 'test'\n",
    "    if i_type == 'ml.p3.16xlarge':\n",
    "        i_tag = 'p3'\n",
    "    elif i_type == 'ml.p3dn.24xlarge':\n",
    "        i_tag = 'p3dn'\n",
    "    elif i_type == 'ml.p4d.24xlarge':\n",
    "        i_tag = 'p4d'    \n",
    "        \n",
    "    trial = \"-\".join([i_tag,str(i_cnt),algo, spot])\n",
    "       \n",
    "    sm_trial = Trial.create(trial_name=f'{experiment_name}-{trial}-{create_date}',\n",
    "                            experiment_name=experiment_name)\n",
    "\n",
    "    job_name = f'{sm_trial.trial_name}'\n",
    "    return job_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 데이터 저장소와 학습 script 위치 설정\n",
    "SageMaker에는 학습에 사용할 데이터 위치와 학습 코드의 위치를 설정합니다. 편의를 위해 default_bucket을 사용했으나, 실제 활용 시에는 이미 생성한 bucket을 활용하는 것도 가능합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'default_bucket' (str)\n"
     ]
    }
   ],
   "source": [
    "prefix = 'ETDataset'\n",
    "\n",
    "sess = boto3.Session()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "sm = sess.client('sagemaker')\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "s3_data_path = f's3://{default_bucket}/{prefix}'\n",
    "source_dir = 'Informer2020'\n",
    "%store default_bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 실험 설정\n",
    "\n",
    "학습 시 사용한 소스코드와 output 정보를 저장할 위치를 선정합니다. 이 값은 필수로 설정하지 않아도 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_location = f's3://{default_bucket}/sm_codes'\n",
    "output_path = f's3://{default_bucket}/poc_informer/output' \n",
    "checkpoint_s3_bucket = f's3://{default_bucket}/checkpoints'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실험에서 표준 출력으로 보여지는 metrics 값을 정규 표현식을 이용하여 SageMaker에서 값을 capture할 수 있습니다. 이 값은 필수로 설정하지 않아도 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions = [\n",
    "    {'Name': 'Epoch', 'Regex': 'Epoch: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?),'},\n",
    "    {'Name': 'train_loss', 'Regex': 'Train Loss: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?),'},\n",
    "    {'Name': 'valid_loss', 'Regex': 'Valid Loss: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?),'},\n",
    "    {'Name': 'test_loss', 'Regex': 'Test Loss: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?),'},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다양한 실험 조건을 테스트하기 위해 hyperparameters로 argument 값들을 노트북에서 설정할 수 있으며, 이 값은 학습 스크립트에서 argument인 변수로 받아서 활용이 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "        'model' : 'informer', # model of experiment, options: [informer, informerstack, informerlight(TBD)]\n",
    "        'data' : 'ETTh1', # data\n",
    "        'root_path' : 'ETT-small/', # root path of data file\n",
    "        'data_path' : 'ETTh1.csv', # data file\n",
    "        'features' : 'M', # forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate\n",
    "        'target' : 'OT', # target feature in S or MS task\n",
    "        'freq' : 'h', # freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h\n",
    "        'checkpoints' : 'informer_checkpoints', # location of model checkpoints\n",
    "\n",
    "        'seq_len' : 96, # input sequence length of Informer encoder\n",
    "        'label_len' : 48, # start token length of Informer decoder\n",
    "        'pred_len' : 24, # prediction sequence length\n",
    "        # Informer decoder input: concat[start token series(label_len), zero padding series(pred_len)]\n",
    "\n",
    "        'enc_in' : 7, # encoder input size\n",
    "        'dec_in' : 7, # decoder input size\n",
    "        'c_out' : 7, # output size\n",
    "        'factor' : 5, # probsparse attn factor\n",
    "        'd_model' : 512, # dimension of model\n",
    "        'n_heads' : 8, # num of heads\n",
    "        'e_layers' : 2, # num of encoder layers\n",
    "        'd_layers' : 1, # num of decoder layers\n",
    "        'd_ff' : 2048, # dimension of fcn in model\n",
    "        'dropout' : 0.05, # dropout\n",
    "        'attn' : 'prob', # attention used in encoder, options:[prob, full]\n",
    "        'embed' : 'timeF', # time features encoding, options:[timeF, fixed, learned]\n",
    "        'activation' : 'gelu', # activation\n",
    "        'distil' : True, # whether to use distilling in encoder\n",
    "        'output_attention' : False, # whether to output attention in ecoder\n",
    "        'mix' : True,\n",
    "        'padding' : 0,\n",
    "        'freq' : 'h',\n",
    "        'do_predict' : True,\n",
    "        'batch_size' : 32,\n",
    "        'learning_rate' : 0.0001,\n",
    "        'loss' : 'mse',\n",
    "        'lradj' : 'type1',\n",
    "        'use_amp' : False, # whether to use automatic mixed precision training\n",
    "\n",
    "        'num_workers' : 0,\n",
    "        'itr' : 1,\n",
    "        'train_epochs' : 1,  ## Training epochs\n",
    "        'patience' : 3,\n",
    "        'des' : 'exp',\n",
    "        'use_multi_gpu' : True\n",
    "    }\n",
    "\n",
    "experiment_name = 'informer-poc-exp1'\n",
    "# instance_type = 'ml.p3.16xlarge'  # 'ml.p3.16xlarge', 'ml.p3dn.24xlarge', 'ml.p4d.24xlarge', 'local_gpu'\n",
    "instance_type = 'ml.c5.4xlarge'\n",
    "instance_count = 1\n",
    "do_spot_training = True\n",
    "max_wait = None\n",
    "max_run = 3*60*60\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "분산학습과 spot 학습을 사용할지를 선정할 수 있습니다. <br>\n",
    "분산학습의 경우 [SageMaker data parallel library](https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel.html)를 사용하고자 할 경우 distribution을 아래와 같이 설정한 후 사용할 수 있습니다. (학습 스크립트 일부 수정 필요) <br>\n",
    "[spot 학습](https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html)을 사용하고자 할 경우 학습 파라미터에 spot 파라미터를 True로 변경한 다음, 자원이 없을 때 대기하는 시간인 max_wait (초)를 설정해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_job_name : informer-dist \n",
      "train_instance_type : ml.c5.4xlarge \n",
      "train_instance_count : 1 \n",
      "image_uri : None \n",
      "distribution : None\n"
     ]
    }
   ],
   "source": [
    "image_uri = None\n",
    "train_job_name = 'sagemaker'\n",
    "\n",
    "\n",
    "train_job_name = 'informer-dist'\n",
    "distribution = {}\n",
    "\n",
    "if instance_type in ['ml.p3.16xlarge', 'ml.p3dn.24xlarge', 'ml.p4d.24xlarge', 'local_gpu']:\n",
    "    distribution[\"smdistributed\"]={ \n",
    "                        \"dataparallel\": {\n",
    "                            \"enabled\": True\n",
    "                        }\n",
    "                }\n",
    "else:\n",
    "    distribution = None\n",
    "\n",
    "if do_spot_training:\n",
    "    max_wait = max_run\n",
    "\n",
    "print(\"train_job_name : {} \\ntrain_instance_type : {} \\ntrain_instance_count : {} \\nimage_uri : {} \\ndistribution : {}\".format(train_job_name, instance_type, instance_count, image_uri, distribution))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pipeline parameters\n",
    "\n",
    "[SageMaker Pipeline](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html)의 중요한 기능은 미리 단계를 정의한 다음, 파이프라인의 재정의 없이도 parameters를 실행 중인 단계에서 변경할 수 있다는 것입니다. parameters를 사용하여 이 작업을 수행할 수 있습니다. <br>\n",
    "ParameterInteger, ParameterFloat, ParameterString를 사용할 수 있으며, 이후 `pipeline.start(parameters=parameters)`를 호출할 때 수정할 수 있는 값을 미리 정의합니다. 특정 parameters만으로 이러한 방식의 정의가 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import CreateModelStep\n",
    "\n",
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep\n",
    "from sagemaker.workflow.parameters import ParameterInteger, ParameterFloat, ParameterString\n",
    "from sagemaker.workflow.conditions import ConditionLessThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep, JsonGet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instance_param = ParameterString(\n",
    "    name=\"TrainingInstance\",\n",
    "    default_value=\"ml.p3.16xlarge\",\n",
    ")\n",
    "\n",
    "train_count_param = ParameterInteger(\n",
    "    name=\"TrainingInstanceCount\",\n",
    "    default_value=1\n",
    ")\n",
    "\n",
    "model_approval_status = ParameterString(\n",
    "    name=\"ModelApprovalStatus\", default_value=\"PendingManualApproval\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 학습을 위한 Estimator 선언\n",
    "\n",
    "AWS 서비스 활용 시 role (역할) 설정은 매우 중요합니다. 이 노트북에서 사용하는 role은 노트북과 training job을 실행할 때 사용하는 role이며, role을 이용하여 다양한 AWS 서비스에 대한 접근 권한을 설정할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::322537213286:role/service-role/AmazonSageMaker-ExecutionRole-20210401T133000'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "role = get_execution_role()\n",
    "role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all input configurations, parameters, and metrics specified in estimator \n",
    "# definition are automatically tracked\n",
    "estimator = PyTorch(\n",
    "    entry_point='main_informer.py',\n",
    "    source_dir=source_dir,\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    framework_version='1.8.1',\n",
    "    py_version='py36',\n",
    "    instance_count=train_count_param,    ## Parameter 값으로 변경\n",
    "    instance_type=train_instance_param,  ## Parameter 값으로 변경\n",
    "    volume_size=256,\n",
    "    code_location = code_location,\n",
    "    output_path=output_path,\n",
    "    hyperparameters=hyperparameters,\n",
    "    distribution=distribution,\n",
    "    metric_definitions=metric_definitions,\n",
    "    max_run=max_run,\n",
    "    checkpoint_s3_uri=checkpoint_s3_bucket,\n",
    "    use_spot_instances=do_spot_training,  # spot instance 활용\n",
    "    max_wait=max_wait,\n",
    "    base_job_name=f\"informer-train\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training 단계 선언\n",
    "\n",
    "[training 단계](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html)를 사용하여 모델을 학습하는 training job을 생성합니다.<br>\n",
    "training 단계에는 estimator, training과 validation 데이터 입력 등이 필요합니다. <br>\n",
    "\n",
    "[caching](https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/pipelines-caching.html) 를 사용하면 SageMaker 파이프라인이 단계를 실행하기 전에 동일한 인수를 사용하여 호출된 단계의 이전 실행을 찾으려고 시도합니다. 파이프라인은 인수가 가리키는 데이터 또는 코드가 변경되었는지 여부를 확인하지 않습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import CacheConfig\n",
    "\n",
    "cache_config = CacheConfig(enable_caching=True, \n",
    "                           expire_after=\"7d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_step = TrainingStep(\n",
    "    name=\"InformerTrain\",\n",
    "    estimator=estimator,\n",
    "    inputs={\n",
    "        \"training\": sagemaker.inputs.TrainingInput(\n",
    "            s3_data=s3_data_path\n",
    "        )\n",
    "    },\n",
    "    cache_config=cache_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Processing 단계 - output에서 압축풀어 test_report.json 가져오기\n",
    "\n",
    "[processing 단계](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.steps.ProcessingStep)는 데이터 처리를 위해 수행되는 processing job을 생성합니다. <br>\n",
    "processing 단계는 processor, processing 코드를 정의하는 python 스크립트, processing을 위한 output, job 관련 arguments 등으로 구성됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n"
     ]
    }
   ],
   "source": [
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=\"0.23-1\",\n",
    "    instance_type=\"ml.c4.xlarge\",\n",
    "    instance_count=1,\n",
    "    base_job_name=f\"GeneratingReport\",  # choose any name\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞선 Train 단계에서의 model 산출물을 postprocessing의 input으로 추가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = ProcessingInput(\n",
    "                        source=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "                        destination=\"/opt/ml/processing/model\",\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_report = PropertyFile(\n",
    "    name=\"TestReport\",\n",
    "    output_name=\"result\",\n",
    "    path=\"test_report.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocessing_step = ProcessingStep(\n",
    "    name=\"PostProcessingforInformer\",  # choose any name\n",
    "    processor=sklearn_processor,\n",
    "    inputs=[model_input],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"result\", source=\"/opt/ml/processing/result\")\n",
    "    ],\n",
    "    code=os.path.join(source_dir, \"postprocess.py\"),\n",
    "    property_files=[test_report],\n",
    "    cache_config=cache_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Register Model\n",
    "\n",
    "[register model 단계](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.step_collections.RegisterModel)를 사용하여 sagemaker.model.Model 또는 sagemaker.pipeline.PipelineModel을 SageMaker의 model registry에 등록합니다. <br>\n",
    "PipelineModel은 inference pipeline을 나타내며, inference 요청을 처리하는 container들의 순서를 구성합니다. <br>\n",
    "register model 단계에서는 등록된 모델의 metrics를 json 구조로 통합하여 등록할 수 있으며, 모델 승인에 대한 방법을 정의할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package_group_name = \"ts-prediction-informer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register model step that will be conditionally executed\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=\"{}/test_report.json\".format(\n",
    "            postprocessing_step.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"],\n",
    "        ),\n",
    "        content_type=\"application/json\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_step = RegisterModel(\n",
    "    name=\"InformerRegisterModel\",\n",
    "    estimator=estimator,\n",
    "    model_data=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[\"ml.m5.xlarge\"],\n",
    "    transform_instances=[\"ml.m5.xlarge\"],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    approval_status=model_approval_status,\n",
    "    model_metrics=model_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Condition 단계\n",
    "\n",
    "[condition 단계](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.condition_step.ConditionStep)를 사용하여 단계 properties의 조건을 평가하여 파이프라인에서 다음에 수행할 작업을 진행할지 여부를 판단합니다. <br>\n",
    "\n",
    "condition 단계에는 condition 목록, condition이 참으로 평가될 경우 실행할 단계 목록, condition이 거짓으로 평가될 경우 실행할 단계 목록 등이 필요합니다. <br>\n",
    "\n",
    "`[제한사항]`\n",
    "- SageMaker 파이프라인은 nested condition 단계의 사용을 지원하지 않습니다. 즉, condition 단계를 다른 조건 단계의 입력으로 전달할 수 없습니다.\n",
    "- condition 단계는 두 개의 분기에서 동일 단계를 사용할 수 없습니다. 즉, 두 개의 분기에서 동일 단계의 기능이 필요한 경우 단계를 복제하고 다른 이름을 지정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.deprecations:The class JsonGet has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "# Condition step for evaluating model quality and branching execution\n",
    "cond_lte = ConditionLessThanOrEqualTo(  # You can change the condition here\n",
    "    left=JsonGet(\n",
    "        step=postprocessing_step,\n",
    "        property_file=test_report,\n",
    "        json_path=\"regression_metrics.mse.value\",  # This should follow the structure of your report_dict defined in the postprocess.py file.\n",
    "    ),\n",
    "    right=1.0,  # You can change the threshold here\n",
    ")\n",
    "cond_step = ConditionStep(\n",
    "    name=\"TestMSECond\",\n",
    "    conditions=[cond_lte],\n",
    "    if_steps=[register_step],\n",
    "    else_steps=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Pipeline 수행\n",
    "\n",
    "지금까지 선언한 Step (단계)를 모두 통합합니다. step의 순서는 DAG을 고려하여 자동으로 정의가 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    name=\"ts-prediction-informer-pipeline\",\n",
    "    parameters=[train_instance_param, train_count_param, model_approval_status],\n",
    "    steps=[\n",
    "        training_step,\n",
    "        postprocessing_step,\n",
    "        cond_step\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker pipeline 서비스에 정의된 pipeline를 제출하게 됩니다. 기존 정의된 동일한 이름의 pipeline이 있는 경우 덮어쓰기가 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: latest.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: latest.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-west-2:322537213286:pipeline/ts-prediction-informer-pipeline',\n",
       " 'ResponseMetadata': {'RequestId': 'acda7d00-bb4e-4c3e-a022-be65447be88f',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'acda7d00-bb4e-4c3e-a022-be65447be88f',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '99',\n",
       "   'date': 'Sun, 26 Sep 2021 10:18:00 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special pipeline parameters can be defined or changed here\n",
    "parameters = {\"TrainingInstance\": \"ml.c5.4xlarge\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_response = pipeline.start(parameters=parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline의 진행 사항을 모니터링할 수 있습니다. wait() 함수는 종료가 될 때까지 대기하기 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-west-2:322537213286:pipeline/ts-prediction-informer-pipeline',\n",
       " 'PipelineExecutionArn': 'arn:aws:sagemaker:us-west-2:322537213286:pipeline/ts-prediction-informer-pipeline/execution/95afzax3d1zd',\n",
       " 'PipelineExecutionDisplayName': 'execution-1632747337746',\n",
       " 'PipelineExecutionStatus': 'Succeeded',\n",
       " 'PipelineExperimentConfig': {'ExperimentName': 'ts-prediction-informer-pipeline',\n",
       "  'TrialName': '95afzax3d1zd'},\n",
       " 'CreationTime': datetime.datetime(2021, 9, 27, 12, 55, 37, 526000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2021, 9, 27, 12, 55, 43, 874000, tzinfo=tzlocal()),\n",
       " 'CreatedBy': {},\n",
       " 'LastModifiedBy': {},\n",
       " 'ResponseMetadata': {'RequestId': '091edc7a-8930-4289-8ce5-8c0b0bf34113',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '091edc7a-8930-4289-8ce5-8c0b0bf34113',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '534',\n",
       "   'date': 'Mon, 27 Sep 2021 12:56:18 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_response.wait()\n",
    "start_response.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "각 step 별로 진행사항을 파악할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'StepName': 'InformerRegisterModel',\n",
       "  'StartTime': datetime.datetime(2021, 9, 26, 10, 33, 0, 251000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2021, 9, 26, 10, 33, 1, 15000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'Metadata': {'RegisterModel': {'Arn': 'arn:aws:sagemaker:us-west-2:322537213286:model-package/ts-prediction-informer/23'}}},\n",
       " {'StepName': 'TestMSECond',\n",
       "  'StartTime': datetime.datetime(2021, 9, 26, 10, 32, 58, 824000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2021, 9, 26, 10, 32, 59, 610000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'Metadata': {'Condition': {'Outcome': 'True'}}},\n",
       " {'StepName': 'PostProcessingforInformer',\n",
       "  'StartTime': datetime.datetime(2021, 9, 26, 10, 28, 5, 641000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2021, 9, 26, 10, 32, 58, 477000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'Metadata': {'ProcessingJob': {'Arn': 'arn:aws:sagemaker:us-west-2:322537213286:processing-job/pipelines-px39815e1ueh-postprocessingforinf-fhq8hyyfop'}}},\n",
       " {'StepName': 'InformerTrain',\n",
       "  'StartTime': datetime.datetime(2021, 9, 26, 10, 18, 1, 513000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2021, 9, 26, 10, 28, 4, 816000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'Metadata': {'TrainingJob': {'Arn': 'arn:aws:sagemaker:us-west-2:322537213286:training-job/pipelines-px39815e1ueh-informertrain-kdnjj7flma'}}}]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_response.list_steps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Model 등록 실행\n",
    "\n",
    "아래 함수를 이용하여 현재 pending 중인 모델을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ModelPackageSummaryList': [{'ModelPackageGroupName': 'ts-prediction-informer',\n",
       "   'ModelPackageVersion': 23,\n",
       "   'ModelPackageArn': 'arn:aws:sagemaker:us-west-2:322537213286:model-package/ts-prediction-informer/23',\n",
       "   'CreationTime': datetime.datetime(2021, 9, 26, 10, 33, 0, 901000, tzinfo=tzlocal()),\n",
       "   'ModelPackageStatus': 'Completed',\n",
       "   'ModelApprovalStatus': 'PendingManualApproval'},\n",
       "  {'ModelPackageGroupName': 'ts-prediction-informer',\n",
       "   'ModelPackageVersion': 18,\n",
       "   'ModelPackageArn': 'arn:aws:sagemaker:us-west-2:322537213286:model-package/ts-prediction-informer/18',\n",
       "   'CreationTime': datetime.datetime(2021, 9, 23, 11, 46, 18, 772000, tzinfo=tzlocal()),\n",
       "   'ModelPackageStatus': 'Completed',\n",
       "   'ModelApprovalStatus': 'PendingManualApproval'},\n",
       "  {'ModelPackageGroupName': 'ts-prediction-informer',\n",
       "   'ModelPackageVersion': 12,\n",
       "   'ModelPackageArn': 'arn:aws:sagemaker:us-west-2:322537213286:model-package/ts-prediction-informer/12',\n",
       "   'CreationTime': datetime.datetime(2021, 9, 23, 6, 24, 58, 208000, tzinfo=tzlocal()),\n",
       "   'ModelPackageStatus': 'Completed',\n",
       "   'ModelApprovalStatus': 'PendingManualApproval'},\n",
       "  {'ModelPackageGroupName': 'ts-prediction-informer',\n",
       "   'ModelPackageVersion': 5,\n",
       "   'ModelPackageArn': 'arn:aws:sagemaker:us-west-2:322537213286:model-package/ts-prediction-informer/5',\n",
       "   'CreationTime': datetime.datetime(2021, 9, 22, 6, 25, 25, 408000, tzinfo=tzlocal()),\n",
       "   'ModelPackageStatus': 'Completed',\n",
       "   'ModelApprovalStatus': 'PendingManualApproval'},\n",
       "  {'ModelPackageGroupName': 'ts-prediction-informer',\n",
       "   'ModelPackageVersion': 1,\n",
       "   'ModelPackageArn': 'arn:aws:sagemaker:us-west-2:322537213286:model-package/ts-prediction-informer/1',\n",
       "   'CreationTime': datetime.datetime(2021, 9, 22, 3, 33, 50, 885000, tzinfo=tzlocal()),\n",
       "   'ModelPackageStatus': 'Completed',\n",
       "   'ModelApprovalStatus': 'PendingManualApproval'}],\n",
       " 'ResponseMetadata': {'RequestId': 'da63d3e4-2b2f-41b0-80c2-097ae300fa98',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'da63d3e4-2b2f-41b0-80c2-097ae300fa98',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '1465',\n",
       "   'date': 'Sun, 26 Sep 2021 10:43:27 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pending_model = sm.list_model_packages(\n",
    "    ModelPackageGroupName=model_package_group_name,\n",
    "    ModelApprovalStatus='PendingManualApproval',\n",
    "    SortBy='Name',\n",
    "    SortOrder='Descending'\n",
    ")\n",
    "pending_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "아래 명령어를 이용하여 실제 승인을 수행하게 됩니다. 아래 예시는 가장 최근 등록된 버전의 모델을 가져와서 승인하도록 만들었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package_update_input_dict = {\n",
    "    \"ModelPackageArn\" : pending_model['ModelPackageSummaryList'][0]['ModelPackageArn'],\n",
    "    \"ModelApprovalStatus\" : \"Approved\"\n",
    "}\n",
    "model_package_update_response = sm.update_model_package(**model_package_update_input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ModelPackageArn': 'arn:aws:sagemaker:us-west-2:322537213286:model-package/ts-prediction-informer/23',\n",
       " 'ResponseMetadata': {'RequestId': '3dd8ade2-1636-427a-9635-0404f67e5ee4',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '3dd8ade2-1636-427a-9635-0404f67e5ee4',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '102',\n",
       "   'date': 'Sun, 26 Sep 2021 10:43:29 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_package_update_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "승인된 모델 중에서 가장 최신 버전의 모델을 검색합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "approved_model = sm.list_model_packages(\n",
    "    ModelPackageGroupName=model_package_group_name,\n",
    "    ModelApprovalStatus='Approved',\n",
    "    SortBy='Name',\n",
    "    SortOrder='Descending'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ModelPackageGroupName': 'ts-prediction-informer',\n",
       " 'ModelPackageVersion': 23,\n",
       " 'ModelPackageArn': 'arn:aws:sagemaker:us-west-2:322537213286:model-package/ts-prediction-informer/23',\n",
       " 'CreationTime': datetime.datetime(2021, 9, 26, 10, 33, 0, 901000, tzinfo=tzlocal()),\n",
       " 'InferenceSpecification': {'Containers': [{'Image': '763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference:1.8.1-gpu-py36',\n",
       "    'ImageDigest': 'sha256:3533e731f4dd08310eaf467d84a7f443d965609f41fb1870c467b16de854f70d',\n",
       "    'ModelDataUrl': 's3://sagemaker-us-west-2-322537213286/poc_informer/output/pipelines-px39815e1ueh-InformerTrain-KdNjJ7FLMa/output/model.tar.gz'}],\n",
       "  'SupportedTransformInstanceTypes': ['ml.m5.xlarge'],\n",
       "  'SupportedRealtimeInferenceInstanceTypes': ['ml.m5.xlarge'],\n",
       "  'SupportedContentTypes': ['text/csv'],\n",
       "  'SupportedResponseMIMETypes': ['text/csv']},\n",
       " 'ModelPackageStatus': 'Completed',\n",
       " 'ModelPackageStatusDetails': {'ValidationStatuses': [],\n",
       "  'ImageScanStatuses': []},\n",
       " 'CertifyForMarketplace': False,\n",
       " 'ModelApprovalStatus': 'Approved',\n",
       " 'MetadataProperties': {'GeneratedBy': 'arn:aws:sagemaker:us-west-2:322537213286:pipeline/ts-prediction-informer-pipeline/execution/px39815e1ueh'},\n",
       " 'ModelMetrics': {'ModelQuality': {'Statistics': {'ContentType': 'application/json',\n",
       "    'S3Uri': 's3://sagemaker-us-west-2-322537213286/GeneratingReport-2021-09-26-10-17-52-774/output/result/test_report.json'}}},\n",
       " 'LastModifiedTime': datetime.datetime(2021, 9, 26, 10, 43, 29, 512000, tzinfo=tzlocal()),\n",
       " 'ResponseMetadata': {'RequestId': 'a202c9c9-1736-4793-9b16-73adc1bbde08',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'a202c9c9-1736-4793-9b16-73adc1bbde08',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '1315',\n",
       "   'date': 'Sun, 26 Sep 2021 10:43:32 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.describe_model_package(ModelPackageName=approved_model['ModelPackageSummaryList'][0]['ModelPackageArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. SageMaker Model 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:sagemaker:us-west-2:322537213286:model-package/ts-prediction-informer/23'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "approved_model['ModelPackageSummaryList'][0]['ModelPackageArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model arn : arn:aws:sagemaker:us-west-2:322537213286:model/ts-prediction-informer\n"
     ]
    }
   ],
   "source": [
    "container_list = [\n",
    "    {\n",
    "        \"ModelPackageName\": approved_model['ModelPackageSummaryList'][0]['ModelPackageArn'], \n",
    "        \"Environment\": {\"SAGEMAKER_PROGRAM\": \"predictor.py\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "try:\n",
    "    sm.delete_model(ModelName=model_package_group_name)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "create_model_response = sm.create_model(\n",
    "    ModelName=model_package_group_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    Containers=container_list\n",
    ")\n",
    "print(\"Model arn : {}\".format(create_model_response[\"ModelArn\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import strftime\n",
    "\n",
    "transform_jobname=model_package_group_name+\"-\"+strftime(\"%m%d-%H%M%s\")\n",
    "\n",
    "response = sm.create_transform_job(\n",
    "  TransformJobName=transform_jobname,\n",
    "  ModelName=model_package_group_name,\n",
    "  MaxConcurrentTransforms=1,\n",
    "  TransformInput={\n",
    "      'DataSource': {\n",
    "          'S3DataSource': {\n",
    "              'S3DataType': 'S3Prefix',\n",
    "              'S3Uri': 's3://sagemaker-us-west-2-322537213286/ETDataset/ETT-small/ETTh1_small.csv'\n",
    "          }\n",
    "      },\n",
    "      'ContentType' : 'text/csv',\n",
    "      'SplitType': 'Line'\n",
    "  },\n",
    "  TransformOutput={\n",
    "      'S3OutputPath': f\"s3://{default_bucket}/batch_result\",\n",
    "      'AssembleWith': 'Line',\n",
    "  },\n",
    "  TransformResources={\n",
    "      'InstanceType': 'ml.m5.xlarge',\n",
    "      'InstanceCount': 1\n",
    "  },\n",
    "  Environment={\n",
    "      'default_bucket': default_bucket\n",
    "  },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TransformJobName': 'ts-prediction-informer',\n",
       " 'TransformJobArn': 'arn:aws:sagemaker:us-west-2:322537213286:transform-job/ts-prediction-informer',\n",
       " 'TransformJobStatus': 'Failed',\n",
       " 'FailureReason': 'AlgorithmError: See job logs for more information',\n",
       " 'ModelName': 'ts-prediction-informer',\n",
       " 'MaxConcurrentTransforms': 1,\n",
       " 'TransformInput': {'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "    'S3Uri': 's3://sagemaker-us-west-2-322537213286/ETDataset/ETT-small/ETTh1_small.csv'}},\n",
       "  'CompressionType': 'None',\n",
       "  'SplitType': 'Line'},\n",
       " 'TransformOutput': {'S3OutputPath': 's3://sagemaker-us-west-2-322537213286/batch_result',\n",
       "  'Accept': 'text/csv',\n",
       "  'AssembleWith': 'None',\n",
       "  'KmsKeyId': ''},\n",
       " 'TransformResources': {'InstanceType': 'ml.m5.xlarge', 'InstanceCount': 1},\n",
       " 'CreationTime': datetime.datetime(2021, 9, 26, 13, 20, 44, 289000, tzinfo=tzlocal()),\n",
       " 'TransformStartTime': datetime.datetime(2021, 9, 26, 13, 23, 59, 397000, tzinfo=tzlocal()),\n",
       " 'TransformEndTime': datetime.datetime(2021, 9, 26, 13, 27, 57, 379000, tzinfo=tzlocal()),\n",
       " 'DataProcessing': {'InputFilter': '$',\n",
       "  'OutputFilter': '$',\n",
       "  'JoinSource': 'None'},\n",
       " 'ResponseMetadata': {'RequestId': 'ca7bdba0-5be2-439c-ab3a-614763a2a7b2',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'ca7bdba0-5be2-439c-ab3a-614763a2a7b2',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '906',\n",
       "   'date': 'Sun, 26 Sep 2021 13:42:31 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.describe_transform_job(TransformJobName=transform_jobname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating transform job with name: ts-prediction-informer-2021-09-26-12-47-27-558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...........................................\u001b[34m2021-09-26 12:54:31,461 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:31,606 [INFO ] main org.pytorch.serve.ModelServer - \u001b[0m\n",
      "\u001b[34mTorchserve version: 0.4.0\u001b[0m\n",
      "\u001b[34mTS Home: /opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mCurrent directory: /\u001b[0m\n",
      "\u001b[34mTemp directory: /home/model-server/tmp\u001b[0m\n",
      "\u001b[34mNumber of GPUs: 0\u001b[0m\n",
      "\u001b[34mNumber of CPUs: 4\u001b[0m\n",
      "\u001b[34mMax heap size: 2972 M\u001b[0m\n",
      "\u001b[34mPython executable: /opt/conda/bin/python3.6\u001b[0m\n",
      "\u001b[34mConfig file: /etc/sagemaker-ts.properties\u001b[0m\n",
      "\u001b[34mInference address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mManagement address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mMetrics address: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[34mModel Store: /.sagemaker/ts/models\u001b[0m\n",
      "\u001b[34mInitial Models: model.mar\u001b[0m\n",
      "\u001b[34mLog dir: /logs\u001b[0m\n",
      "\u001b[34mMetrics dir: /logs\u001b[0m\n",
      "\u001b[34mNetty threads: 0\u001b[0m\n",
      "\u001b[34mNetty client threads: 0\u001b[0m\n",
      "\u001b[34mDefault workers per model: 4\u001b[0m\n",
      "\u001b[34mBlacklist Regex: N/A\u001b[0m\n",
      "\u001b[34mMaximum Response Size: 6553500\u001b[0m\n",
      "\u001b[34mMaximum Request Size: 6553500\u001b[0m\n",
      "\u001b[34mPrefer direct buffer: false\u001b[0m\n",
      "\u001b[34mAllowed Urls: [file://.*|http(s)?://.*]\u001b[0m\n",
      "\u001b[34mCustom python dependency for model allowed: false\u001b[0m\n",
      "\u001b[34mMetrics report format: prometheus\u001b[0m\n",
      "\u001b[34mEnable metrics API: true\u001b[0m\n",
      "\u001b[34mWorkflow Store: /.sagemaker/ts/models\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:31,615 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:31,643 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: model.mar\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:32,932 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:32,958 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:33,266 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:33,267 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:33,285 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:33,681 [INFO ] pool-1-thread-5 ACCESS_LOG - /169.254.255.130:47258 \"GET /ping HTTP/1.1\" 200 48\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:33,689 [INFO ] pool-1-thread-5 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:d32ef17d544b,timestamp:null\u001b[0m\n",
      "\u001b[34mModel server started.\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:33,826 [WARN ] pool-2-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:33,949 [INFO ] pool-2-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:d32ef17d544b,timestamp:1632660873\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:33,950 [INFO ] pool-2-thread-1 TS_METRICS - DiskAvailable.Gigabytes:38.225162506103516|#Level:Host|#hostname:d32ef17d544b,timestamp:1632660873\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:33,950 [INFO ] pool-2-thread-1 TS_METRICS - DiskUsage.Gigabytes:17.690750122070312|#Level:Host|#hostname:d32ef17d544b,timestamp:1632660873\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:33,951 [INFO ] pool-2-thread-1 TS_METRICS - DiskUtilization.Percent:31.6|#Level:Host|#hostname:d32ef17d544b,timestamp:1632660873\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:33,951 [INFO ] pool-2-thread-1 TS_METRICS - MemoryAvailable.Megabytes:14062.20703125|#Level:Host|#hostname:d32ef17d544b,timestamp:1632660873\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:33,951 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUsed.Megabytes:1208.76171875|#Level:Host|#hostname:d32ef17d544b,timestamp:1632660873\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:33,952 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUtilization.Percent:9.7|#Level:Host|#hostname:d32ef17d544b,timestamp:1632660873\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,072 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:47262 \"GET /execution-parameters HTTP/1.1\" 404 1\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,072 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:d32ef17d544b,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,105 [INFO ] W-9000-model_1-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,106 [INFO ] W-9000-model_1-stdout MODEL_LOG - [PID]49\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,106 [INFO ] W-9000-model_1-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,107 [INFO ] W-9000-model_1-stdout MODEL_LOG - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,110 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,126 [INFO ] W-9000-model_1-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,407 [INFO ] W-9003-model_1-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,410 [INFO ] W-9003-model_1-stdout MODEL_LOG - [PID]50\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,411 [INFO ] W-9003-model_1-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,411 [INFO ] W-9003-model_1-stdout MODEL_LOG - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,409 [INFO ] W-9002-model_1-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,411 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,412 [INFO ] W-9002-model_1-stdout MODEL_LOG - [PID]52\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,415 [INFO ] W-9002-model_1-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,415 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,416 [INFO ] W-9002-model_1-stdout MODEL_LOG - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,426 [INFO ] W-9003-model_1-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,467 [INFO ] W-9002-model_1-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,532 [INFO ] W-9001-model_1-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,533 [INFO ] W-9001-model_1-stdout MODEL_LOG - [PID]51\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,533 [INFO ] W-9001-model_1-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,533 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,533 [INFO ] W-9001-model_1-stdout MODEL_LOG - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,549 [INFO ] W-9001-model_1-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,576 [INFO ] W-9000-model_1-stdout MODEL_LOG - Generating new fontManager, this may take some time...\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:35,004 [INFO ] W-9003-model_1-stdout MODEL_LOG - Generating new fontManager, this may take some time...\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:35,051 [INFO ] W-9002-model_1-stdout MODEL_LOG - Generating new fontManager, this may take some time...\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:35,136 [INFO ] W-9001-model_1-stdout MODEL_LOG - Generating new fontManager, this may take some time...\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:35,945 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1771\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:35,949 [INFO ] W-9000-model_1 TS_METRICS - W-9000-model_1.ms:3011|#Level:Host|#hostname:d32ef17d544b,timestamp:1632660875\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:35,949 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:39|#Level:Host|#hostname:d32ef17d544b,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:36,217 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1596\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:36,217 [INFO ] W-9001-model_1 TS_METRICS - W-9001-model_1.ms:3277|#Level:Host|#hostname:d32ef17d544b,timestamp:1632660876\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:36,217 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:72|#Level:Host|#hostname:d32ef17d544b,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:36,235 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1751\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:36,236 [INFO ] W-9003-model_1 TS_METRICS - W-9003-model_1.ms:3293|#Level:Host|#hostname:d32ef17d544b,timestamp:1632660876\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:36,236 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:60|#Level:Host|#hostname:d32ef17d544b,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:36,253 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1738\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:36,253 [INFO ] W-9002-model_1 TS_METRICS - W-9002-model_1.ms:3310|#Level:Host|#hostname:d32ef17d544b,timestamp:1632660876\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:36,253 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:48|#Level:Host|#hostname:d32ef17d544b,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:37,286 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1335\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:37,286 [INFO ] W-9000-model_1-stdout MODEL_LOG - Informer Model loaded\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:37,286 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:47292 \"POST /invocations HTTP/1.1\" 200 1822\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:37,286 [INFO ] W-9000-model_1-stdout MODEL_LOG - default_bucket : sagemaker-us-west-2-322537213286\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:37,286 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:d32ef17d544b,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:37,287 [INFO ] W-9000-model_1-stdout MODEL_LOG - df_raw.columns : Index(['date', 'HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL', 'OT'], dtype='object')\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:37,287 [INFO ] W-9000-model_1-stdout MODEL_LOG - self.target : OT\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:37,287 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:466|#Level:Host|#hostname:d32ef17d544b,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:37,287 [INFO ] W-9000-model_1-stdout MODEL_LOG - Syncing files from prediciton_result.csv to s3://sagemaker-us-west-2-322537213286/prediciton_result/\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:37,287 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:d32ef17d544b,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:37,287 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1333.57|#ModelName:model,Level:Model|#hostname:d32ef17d544b,requestID:21cf303d-8ac8-4ab4-a9e3-aecac4c3aa54,timestamp:1632660877\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:37,286 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1335\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:37,286 [INFO ] W-9000-model_1-stdout MODEL_LOG - Informer Model loaded\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:37,286 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:47292 \"POST /invocations HTTP/1.1\" 200 1822\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:37,286 [INFO ] W-9000-model_1-stdout MODEL_LOG - default_bucket : sagemaker-us-west-2-322537213286\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:37,286 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:d32ef17d544b,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:37,287 [INFO ] W-9000-model_1-stdout MODEL_LOG - df_raw.columns : Index(['date', 'HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL', 'OT'], dtype='object')\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:37,287 [INFO ] W-9000-model_1-stdout MODEL_LOG - self.target : OT\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:37,287 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:466|#Level:Host|#hostname:d32ef17d544b,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:37,287 [INFO ] W-9000-model_1-stdout MODEL_LOG - Syncing files from prediciton_result.csv to s3://sagemaker-us-west-2-322537213286/prediciton_result/\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:37,287 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:d32ef17d544b,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:37,287 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1333.57|#ModelName:model,Level:Model|#hostname:d32ef17d544b,requestID:21cf303d-8ac8-4ab4-a9e3-aecac4c3aa54,timestamp:1632660877\u001b[0m\n",
      "\u001b[32m2021-09-26T12:54:34.082:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\n",
      "\u001b[34m2021-09-26 12:54:31,461 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:31,606 [INFO ] main org.pytorch.serve.ModelServer - \u001b[0m\n",
      "\u001b[34mTorchserve version: 0.4.0\u001b[0m\n",
      "\u001b[34mTS Home: /opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mCurrent directory: /\u001b[0m\n",
      "\u001b[34mTemp directory: /home/model-server/tmp\u001b[0m\n",
      "\u001b[34mNumber of GPUs: 0\u001b[0m\n",
      "\u001b[34mNumber of CPUs: 4\u001b[0m\n",
      "\u001b[34mMax heap size: 2972 M\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:31,461 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:31,606 [INFO ] main org.pytorch.serve.ModelServer - \u001b[0m\n",
      "\u001b[35mTorchserve version: 0.4.0\u001b[0m\n",
      "\u001b[35mTS Home: /opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[35mCurrent directory: /\u001b[0m\n",
      "\u001b[35mTemp directory: /home/model-server/tmp\u001b[0m\n",
      "\u001b[35mNumber of GPUs: 0\u001b[0m\n",
      "\u001b[35mNumber of CPUs: 4\u001b[0m\n",
      "\u001b[35mMax heap size: 2972 M\u001b[0m\n",
      "\u001b[34mPython executable: /opt/conda/bin/python3.6\u001b[0m\n",
      "\u001b[34mConfig file: /etc/sagemaker-ts.properties\u001b[0m\n",
      "\u001b[34mInference address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mManagement address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mMetrics address: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[34mModel Store: /.sagemaker/ts/models\u001b[0m\n",
      "\u001b[34mInitial Models: model.mar\u001b[0m\n",
      "\u001b[34mLog dir: /logs\u001b[0m\n",
      "\u001b[34mMetrics dir: /logs\u001b[0m\n",
      "\u001b[34mNetty threads: 0\u001b[0m\n",
      "\u001b[34mNetty client threads: 0\u001b[0m\n",
      "\u001b[34mDefault workers per model: 4\u001b[0m\n",
      "\u001b[34mBlacklist Regex: N/A\u001b[0m\n",
      "\u001b[34mMaximum Response Size: 6553500\u001b[0m\n",
      "\u001b[34mMaximum Request Size: 6553500\u001b[0m\n",
      "\u001b[34mPrefer direct buffer: false\u001b[0m\n",
      "\u001b[34mAllowed Urls: [file://.*|http(s)?://.*]\u001b[0m\n",
      "\u001b[34mCustom python dependency for model allowed: false\u001b[0m\n",
      "\u001b[34mMetrics report format: prometheus\u001b[0m\n",
      "\u001b[34mEnable metrics API: true\u001b[0m\n",
      "\u001b[34mWorkflow Store: /.sagemaker/ts/models\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:31,615 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:31,643 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: model.mar\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:32,932 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:32,958 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[35mPython executable: /opt/conda/bin/python3.6\u001b[0m\n",
      "\u001b[35mConfig file: /etc/sagemaker-ts.properties\u001b[0m\n",
      "\u001b[35mInference address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[35mManagement address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[35mMetrics address: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[35mModel Store: /.sagemaker/ts/models\u001b[0m\n",
      "\u001b[35mInitial Models: model.mar\u001b[0m\n",
      "\u001b[35mLog dir: /logs\u001b[0m\n",
      "\u001b[35mMetrics dir: /logs\u001b[0m\n",
      "\u001b[35mNetty threads: 0\u001b[0m\n",
      "\u001b[35mNetty client threads: 0\u001b[0m\n",
      "\u001b[35mDefault workers per model: 4\u001b[0m\n",
      "\u001b[35mBlacklist Regex: N/A\u001b[0m\n",
      "\u001b[35mMaximum Response Size: 6553500\u001b[0m\n",
      "\u001b[35mMaximum Request Size: 6553500\u001b[0m\n",
      "\u001b[35mPrefer direct buffer: false\u001b[0m\n",
      "\u001b[35mAllowed Urls: [file://.*|http(s)?://.*]\u001b[0m\n",
      "\u001b[35mCustom python dependency for model allowed: false\u001b[0m\n",
      "\u001b[35mMetrics report format: prometheus\u001b[0m\n",
      "\u001b[35mEnable metrics API: true\u001b[0m\n",
      "\u001b[35mWorkflow Store: /.sagemaker/ts/models\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:31,615 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:31,643 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: model.mar\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:32,932 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:32,958 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:33,266 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:33,267 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:33,285 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:33,681 [INFO ] pool-1-thread-5 ACCESS_LOG - /169.254.255.130:47258 \"GET /ping HTTP/1.1\" 200 48\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:33,689 [INFO ] pool-1-thread-5 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:d32ef17d544b,timestamp:null\u001b[0m\n",
      "\u001b[34mModel server started.\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:33,826 [WARN ] pool-2-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:33,949 [INFO ] pool-2-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:d32ef17d544b,timestamp:1632660873\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:33,950 [INFO ] pool-2-thread-1 TS_METRICS - DiskAvailable.Gigabytes:38.225162506103516|#Level:Host|#hostname:d32ef17d544b,timestamp:1632660873\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:33,950 [INFO ] pool-2-thread-1 TS_METRICS - DiskUsage.Gigabytes:17.690750122070312|#Level:Host|#hostname:d32ef17d544b,timestamp:1632660873\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:33,951 [INFO ] pool-2-thread-1 TS_METRICS - DiskUtilization.Percent:31.6|#Level:Host|#hostname:d32ef17d544b,timestamp:1632660873\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:33,951 [INFO ] pool-2-thread-1 TS_METRICS - MemoryAvailable.Megabytes:14062.20703125|#Level:Host|#hostname:d32ef17d544b,timestamp:1632660873\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:33,951 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUsed.Megabytes:1208.76171875|#Level:Host|#hostname:d32ef17d544b,timestamp:1632660873\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:33,952 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUtilization.Percent:9.7|#Level:Host|#hostname:d32ef17d544b,timestamp:1632660873\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:33,266 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:33,267 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:33,285 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:33,681 [INFO ] pool-1-thread-5 ACCESS_LOG - /169.254.255.130:47258 \"GET /ping HTTP/1.1\" 200 48\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:33,689 [INFO ] pool-1-thread-5 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:d32ef17d544b,timestamp:null\u001b[0m\n",
      "\u001b[35mModel server started.\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:33,826 [WARN ] pool-2-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:33,949 [INFO ] pool-2-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:d32ef17d544b,timestamp:1632660873\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:33,950 [INFO ] pool-2-thread-1 TS_METRICS - DiskAvailable.Gigabytes:38.225162506103516|#Level:Host|#hostname:d32ef17d544b,timestamp:1632660873\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:33,950 [INFO ] pool-2-thread-1 TS_METRICS - DiskUsage.Gigabytes:17.690750122070312|#Level:Host|#hostname:d32ef17d544b,timestamp:1632660873\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:33,951 [INFO ] pool-2-thread-1 TS_METRICS - DiskUtilization.Percent:31.6|#Level:Host|#hostname:d32ef17d544b,timestamp:1632660873\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:33,951 [INFO ] pool-2-thread-1 TS_METRICS - MemoryAvailable.Megabytes:14062.20703125|#Level:Host|#hostname:d32ef17d544b,timestamp:1632660873\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:33,951 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUsed.Megabytes:1208.76171875|#Level:Host|#hostname:d32ef17d544b,timestamp:1632660873\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:33,952 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUtilization.Percent:9.7|#Level:Host|#hostname:d32ef17d544b,timestamp:1632660873\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,072 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:47262 \"GET /execution-parameters HTTP/1.1\" 404 1\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,072 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:d32ef17d544b,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,105 [INFO ] W-9000-model_1-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:34,072 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:47262 \"GET /execution-parameters HTTP/1.1\" 404 1\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:34,072 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:d32ef17d544b,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:34,105 [INFO ] W-9000-model_1-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,106 [INFO ] W-9000-model_1-stdout MODEL_LOG - [PID]49\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,106 [INFO ] W-9000-model_1-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,107 [INFO ] W-9000-model_1-stdout MODEL_LOG - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,110 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,126 [INFO ] W-9000-model_1-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,407 [INFO ] W-9003-model_1-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,410 [INFO ] W-9003-model_1-stdout MODEL_LOG - [PID]50\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,411 [INFO ] W-9003-model_1-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,411 [INFO ] W-9003-model_1-stdout MODEL_LOG - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,409 [INFO ] W-9002-model_1-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,411 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,412 [INFO ] W-9002-model_1-stdout MODEL_LOG - [PID]52\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,415 [INFO ] W-9002-model_1-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,415 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,416 [INFO ] W-9002-model_1-stdout MODEL_LOG - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,426 [INFO ] W-9003-model_1-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,467 [INFO ] W-9002-model_1-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,532 [INFO ] W-9001-model_1-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,533 [INFO ] W-9001-model_1-stdout MODEL_LOG - [PID]51\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,533 [INFO ] W-9001-model_1-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,533 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,533 [INFO ] W-9001-model_1-stdout MODEL_LOG - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,549 [INFO ] W-9001-model_1-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:34,576 [INFO ] W-9000-model_1-stdout MODEL_LOG - Generating new fontManager, this may take some time...\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:35,004 [INFO ] W-9003-model_1-stdout MODEL_LOG - Generating new fontManager, this may take some time...\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:34,106 [INFO ] W-9000-model_1-stdout MODEL_LOG - [PID]49\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:34,106 [INFO ] W-9000-model_1-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:34,107 [INFO ] W-9000-model_1-stdout MODEL_LOG - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:34,110 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:34,126 [INFO ] W-9000-model_1-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:34,407 [INFO ] W-9003-model_1-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:34,410 [INFO ] W-9003-model_1-stdout MODEL_LOG - [PID]50\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:34,411 [INFO ] W-9003-model_1-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:34,411 [INFO ] W-9003-model_1-stdout MODEL_LOG - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:34,409 [INFO ] W-9002-model_1-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:34,411 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:34,412 [INFO ] W-9002-model_1-stdout MODEL_LOG - [PID]52\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:34,415 [INFO ] W-9002-model_1-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:34,415 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:34,416 [INFO ] W-9002-model_1-stdout MODEL_LOG - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:34,426 [INFO ] W-9003-model_1-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:34,467 [INFO ] W-9002-model_1-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:34,532 [INFO ] W-9001-model_1-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:34,533 [INFO ] W-9001-model_1-stdout MODEL_LOG - [PID]51\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:34,533 [INFO ] W-9001-model_1-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:34,533 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:34,533 [INFO ] W-9001-model_1-stdout MODEL_LOG - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:34,549 [INFO ] W-9001-model_1-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:34,576 [INFO ] W-9000-model_1-stdout MODEL_LOG - Generating new fontManager, this may take some time...\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:35,004 [INFO ] W-9003-model_1-stdout MODEL_LOG - Generating new fontManager, this may take some time...\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:35,051 [INFO ] W-9002-model_1-stdout MODEL_LOG - Generating new fontManager, this may take some time...\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:35,136 [INFO ] W-9001-model_1-stdout MODEL_LOG - Generating new fontManager, this may take some time...\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:35,945 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1771\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:35,949 [INFO ] W-9000-model_1 TS_METRICS - W-9000-model_1.ms:3011|#Level:Host|#hostname:d32ef17d544b,timestamp:1632660875\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:35,949 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:39|#Level:Host|#hostname:d32ef17d544b,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:35,051 [INFO ] W-9002-model_1-stdout MODEL_LOG - Generating new fontManager, this may take some time...\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:35,136 [INFO ] W-9001-model_1-stdout MODEL_LOG - Generating new fontManager, this may take some time...\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:35,945 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1771\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:35,949 [INFO ] W-9000-model_1 TS_METRICS - W-9000-model_1.ms:3011|#Level:Host|#hostname:d32ef17d544b,timestamp:1632660875\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:35,949 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:39|#Level:Host|#hostname:d32ef17d544b,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:36,217 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1596\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:36,217 [INFO ] W-9001-model_1 TS_METRICS - W-9001-model_1.ms:3277|#Level:Host|#hostname:d32ef17d544b,timestamp:1632660876\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:36,217 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:72|#Level:Host|#hostname:d32ef17d544b,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:36,235 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1751\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:36,236 [INFO ] W-9003-model_1 TS_METRICS - W-9003-model_1.ms:3293|#Level:Host|#hostname:d32ef17d544b,timestamp:1632660876\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:36,236 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:60|#Level:Host|#hostname:d32ef17d544b,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:36,253 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1738\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:36,253 [INFO ] W-9002-model_1 TS_METRICS - W-9002-model_1.ms:3310|#Level:Host|#hostname:d32ef17d544b,timestamp:1632660876\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:36,217 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1596\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:36,217 [INFO ] W-9001-model_1 TS_METRICS - W-9001-model_1.ms:3277|#Level:Host|#hostname:d32ef17d544b,timestamp:1632660876\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:36,217 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:72|#Level:Host|#hostname:d32ef17d544b,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:36,235 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1751\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:36,236 [INFO ] W-9003-model_1 TS_METRICS - W-9003-model_1.ms:3293|#Level:Host|#hostname:d32ef17d544b,timestamp:1632660876\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:36,236 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:60|#Level:Host|#hostname:d32ef17d544b,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:36,253 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1738\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:36,253 [INFO ] W-9002-model_1 TS_METRICS - W-9002-model_1.ms:3310|#Level:Host|#hostname:d32ef17d544b,timestamp:1632660876\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:36,253 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:48|#Level:Host|#hostname:d32ef17d544b,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:36,253 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:48|#Level:Host|#hostname:d32ef17d544b,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:37,286 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1335\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:37,286 [INFO ] W-9000-model_1-stdout MODEL_LOG - Informer Model loaded\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:37,286 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:47292 \"POST /invocations HTTP/1.1\" 200 1822\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:37,286 [INFO ] W-9000-model_1-stdout MODEL_LOG - default_bucket : sagemaker-us-west-2-322537213286\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:37,286 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:d32ef17d544b,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:37,287 [INFO ] W-9000-model_1-stdout MODEL_LOG - df_raw.columns : Index(['date', 'HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL', 'OT'], dtype='object')\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:37,287 [INFO ] W-9000-model_1-stdout MODEL_LOG - self.target : OT\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:37,287 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:466|#Level:Host|#hostname:d32ef17d544b,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:37,287 [INFO ] W-9000-model_1-stdout MODEL_LOG - Syncing files from prediciton_result.csv to s3://sagemaker-us-west-2-322537213286/prediciton_result/\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:37,287 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:d32ef17d544b,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-09-26 12:54:37,287 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1333.57|#ModelName:model,Level:Model|#hostname:d32ef17d544b,requestID:21cf303d-8ac8-4ab4-a9e3-aecac4c3aa54,timestamp:1632660877\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:37,286 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1335\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:37,286 [INFO ] W-9000-model_1-stdout MODEL_LOG - Informer Model loaded\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:37,286 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:47292 \"POST /invocations HTTP/1.1\" 200 1822\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:37,286 [INFO ] W-9000-model_1-stdout MODEL_LOG - default_bucket : sagemaker-us-west-2-322537213286\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:37,286 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:d32ef17d544b,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:37,287 [INFO ] W-9000-model_1-stdout MODEL_LOG - df_raw.columns : Index(['date', 'HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL', 'OT'], dtype='object')\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:37,287 [INFO ] W-9000-model_1-stdout MODEL_LOG - self.target : OT\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:37,287 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:466|#Level:Host|#hostname:d32ef17d544b,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:37,287 [INFO ] W-9000-model_1-stdout MODEL_LOG - Syncing files from prediciton_result.csv to s3://sagemaker-us-west-2-322537213286/prediciton_result/\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:37,287 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:d32ef17d544b,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-09-26 12:54:37,287 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1333.57|#ModelName:model,Level:Model|#hostname:d32ef17d544b,requestID:21cf303d-8ac8-4ab4-a9e3-aecac4c3aa54,timestamp:1632660877\u001b[0m\n",
      "\u001b[32m2021-09-26T12:54:34.082:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.transformer import Transformer\n",
    "\n",
    "transformer = Transformer(\n",
    "    model_package_group_name,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    output_path=f\"s3://{default_bucket}/batch_result\",\n",
    "    env={\n",
    "        \"default_bucket\" : default_bucket\n",
    "    },\n",
    "    sagemaker_session=sagemaker_session,\n",
    "#     strategy=\"MultiRecord\",\n",
    "    assemble_with=\"Line\",\n",
    ")\n",
    "test_s3 = 's3://sagemaker-us-west-2-322537213286/ETDataset/ETT-small/ETTh1_small.csv'\n",
    "transformer.transform(test_s3, content_type=\"text/csv\", split_type=\"Line\")\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.c5.large",
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
